import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
import os
from glob import glob
from PIL import Image
import onnxruntime as ort
from sklearn.metrics import precision_score, recall_score, f1_score
import time
import albumentations as A
from albumentations.pytorch import ToTensorV2
import torch.backends.cudnn
import gzip

# ====================== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ======================
class Config:
    # –≤—ã–±–æ—Ä —Å–∏—Å—Ç–µ–º—ã
    dir = "/content/NeuralNetwork-ImageClassifier/"

    # –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º –∏ –º–æ–¥–µ–ª—è–º
    source_dir = dir + "DataSet/ARTS/Original"         # –ü–∞–ø–∫–∞ —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
    checkpoint_path = dir + "Model/best_model.pth"     # –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è/–∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
    labels_path = dir + "Model/labels.txt"             # –§–∞–π–ª —Å –º–µ—Ç–∫–∞–º–∏ –∫–ª–∞—Å—Å–æ–≤
    onnx_path = dir + "Model/model.onnx"               # –ü—É—Ç—å –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –≤ ONNX —Ñ–æ—Ä–º–∞—Ç–µ

    # –§–ª–∞–≥–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏–µ–º
    resume_training = False         # –ü—Ä–æ–¥–æ–ª–∂–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞, –µ—Å–ª–∏ True

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    input_size = (224, 224)         # –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (—à–∏—Ä–∏–Ω–∞, –≤—ã—Å–æ—Ç–∞)

    # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ –∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    num_experts = 64                # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –≤ MoE (Mixture of Experts)
    expert_units = 1024             # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ –∫–∞–∂–¥–æ–º —ç–∫—Å–ø–µ—Ä—Ç–µ
    k_top_expert = 4                # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –Ω–∞ –æ–¥–∏–Ω –ø—Ä–∏–º–µ—Ä
    se_reduction = 16               # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Ä–µ–¥—É–∫—Ü–∏–∏ –¥–ª—è SE (Squeeze-and-Excitation) –±–ª–æ–∫–∞
    dropout = 0.5                   # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—Ç–∫–ª—é—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–æ–≤ (dropout)

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    val_split = 0.2                 # –î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö, –≤—ã–¥–µ–ª—è–µ–º–∞—è –ø–æ–¥ –≤–∞–ª–∏–¥–∞—Ü–∏—é
    gradient_clip = 1.0             # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞
    batch_size = 256                # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (—á–∏—Å–ª–æ –ø—Ä–∏–º–µ—Ä–æ–≤, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã—Ö –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥)
    epochs = 100                    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è (–ø–æ–ª–Ω—ã—Ö –ø—Ä–æ—Ö–æ–¥–æ–≤ –ø–æ –≤—Å–µ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É)
    focal_gamma = 5                 # –ü–∞—Ä–∞–º–µ—Ç—Ä –≥–∞–º–º–∞ –¥–ª—è Focal Loss, —Ä–µ–≥—É–ª–∏—Ä—É–µ—Ç —Å—Ç–µ–ø–µ–Ω—å —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–∏ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö
    smoothing = 0.1                 # –ü–∞—Ä–∞–º–µ—Ç—Ä label smoothing, –∑–∞–¥–∞—ë—Ç —É—Ä–æ–≤–µ–Ω—å —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è –º–µ—Ç–æ–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±–æ–±—â–µ–Ω–∏—è
    mixed_precision = True          # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–º–µ—à–∞–Ω–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å (fp16) –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã LR
    max_lr = 0.005                  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (learning rate)
    ini_lr = 0.001                  # –ù–∞—á–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
    plateau_factor = 0.9            # –£–º–µ–Ω—å—à–∞—Ç—å lr
    plateau_threshold = 0.01        # –ü–æ—Ä–æ–≥ —É–ª—É—á—à–µ–Ω–∏—è (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π)
    early_stopping_patience = 5     # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è –¥–ª—è —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏

config = Config()

# ====================== –ö–û–°–ò–ù–£–°–ù–´–ô –®–ï–î–£–õ–ï–† –° WARMUP ======================
class WarmupReduceLROnPlateau():
    """Warmup + ReduceLROnPlateau –ª–æ–≥–∏–∫–∞"""

    def __init__(self, optimizer, ini_lr, max_lr, factor, threshold):
        self.optimizer = optimizer

        self.max_lr = max_lr
        self.ini_lr = ini_lr
        self.current_epoch = 0

        # ReduceLROnPlateau –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.factor = factor
        self.threshold = threshold
        self.num_reduced = 0

        # –¢—Ä–µ–∫–∏–Ω–≥ –ª—É—á—à–µ–≥–æ loss
        self.best_loss = float('inf')

    def step(self, epoch=None, validation_loss=None):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Ü–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏ —Å validation_loss"""
        if epoch is not None:
            self.current_epoch = epoch
        else:
            self.current_epoch += 1

        if self.current_epoch == 1:
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = self.ini_lr
            self.best_loss = validation_loss
            return self.optimizer.param_groups[0]['lr']

        # Warmup —Ñ–∞–∑–∞
        if self.current_epoch == 2:
            factor = self.max_lr / self.ini_lr
            self._change_lr(factor)
            self.best_loss = validation_loss
            return self.optimizer.param_groups[0]['lr']

        if self._is_better(validation_loss, self.best_loss):
            self.best_loss = validation_loss
            print(f"‚úì –£–ª—É—á—à–µ–Ω–∏–µ!")
        else:
            self._reduce_lr()
            print(f"üìâ –£–º–µ–Ω—å—à–µ–Ω–∏–µ LR! –ù–æ–≤—ã–π LR: {self.optimizer.param_groups[0]['lr']:.6f}")

        return self.optimizer.param_groups[0]['lr']

    def _is_better(self, current, best):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, –ª—É—á—à–µ –ª–∏ —Ç–µ–∫—É—â–∏–π loss"""
        return current < best - best * self.threshold

    def _reduce_lr(self):
        """–£–º–µ–Ω—å—à–µ–Ω–∏–µ LR –¥–ª—è –≤—Å–µ—Ö –≥—Ä—É–ø–ø –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        self.num_reduced += 1
        factor = self.factor**self.num_reduced
        print(f"[LR]    Factor:    {factor:.8f}")
        self._change_lr(factor)

    def _change_lr(self, factor):
        for param_group in self.optimizer.param_groups:
            old_lr = param_group['lr']
            new_lr = old_lr * factor
            param_group['lr'] = new_lr

    def get_last_lr(self):
        return self.optimizer.param_groups[0]['lr']

    def state_dict(self):
        return {
            'current_epoch': self.current_epoch,
            'ini_lr': self.ini_lr,
            'max_lr': self.max_lr,
            'best_loss': self.best_loss,
            'factor': self.factor,
            'threshold': self.threshold,
        }

    def load_state_dict(self, state_dict):
        for key, value in state_dict.items():
            setattr(self, key, value)

# ====================== –ö–û–ú–ü–û–ù–ï–ù–¢–´ –ú–û–î–ï–õ–ò ======================
class MoE(nn.Module):
    def __init__(self, input_dim, num_experts, base_expert_units, k_top):
        super().__init__()
        self.num_experts = num_experts
        self.k_top = k_top
        self.experts = nn.ModuleList()

        # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ä–∞–∑–º–µ—Ä–æ–≤ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –æ—Ç 0.5 –¥–æ 1.5 –æ—Ç –±–∞–∑–æ–≤–æ–≥–æ
        expert_sizes = []
        for i in range(num_experts):
            # –õ–∏–Ω–µ–π–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –æ—Ç 0.5 –¥–æ 1.5
            scale = 0.5 + (i / (num_experts - 1)) if num_experts > 1 else 1.0
            size = int(base_expert_units * scale)
            expert_sizes.append(size)

        print(f"MoE expert sizes: {expert_sizes}")

        for size in expert_sizes:
            self.experts.append(nn.Sequential(
                nn.Linear(input_dim, size),
                nn.BatchNorm1d(size),
                nn.ReLU(inplace=True),
                nn.Dropout(config.dropout),
                nn.Linear(size, input_dim),
                nn.BatchNorm1d(input_dim)
            ))

        self.router = nn.Linear(input_dim, num_experts)

    def forward(self, x):
        logits = self.router(x)
        top_k_weights, top_k_indices = logits.topk(self.k_top, dim=1)
        top_k_weights = torch.softmax(top_k_weights, dim=1)

        # –°–æ–±–∏—Ä–∞–µ–º –≤—ã—Ö–æ–¥—ã –≤—Å–µ—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤
        expert_outputs = []
        for expert in self.experts:
            expert_outputs.append(expert(x))
        expert_outputs = torch.stack(expert_outputs, dim=1)  # [B, num_experts, D]

        # –°–æ–∑–¥–∞–µ–º –º–∞—Å–∫—É –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤
        mask = torch.zeros_like(expert_outputs)
        mask = torch.scatter(
            mask,
            1,
            top_k_indices.unsqueeze(-1).expand(-1, -1, expert_outputs.size(-1)),
            1.0
        )

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Ç–æ–ª—å–∫–æ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤
        expert_outputs = expert_outputs * mask + (expert_outputs * (1 - mask)).detach()

        # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ø-k —ç–∫—Å–ø–µ—Ä—Ç–æ–≤
        selected_outputs = expert_outputs.gather(
            1,
            top_k_indices.unsqueeze(-1).expand(-1, -1, expert_outputs.size(-1))
        )

        # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ
        output = (selected_outputs * top_k_weights.unsqueeze(-1)).sum(dim=1)
        return output + x

class ECABlock(nn.Module):
    def __init__(self, channels, k_size=3):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        # 1D —Å–≤—ë—Ä—Ç–∫–∞ –ø–æ –∫–∞–Ω–∞–ª–∞–º
        self.conv = nn.Conv1d(1, 1, kernel_size=k_size,
                              padding=(k_size - 1) // 2, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        y = self.avg_pool(x)  # [B, C, 1, 1]
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º—É [B, 1, C] –¥–ª—è 1D conv
        y = self.conv(y.squeeze(-1).transpose(-1, -2))
        y = self.sigmoid(y).transpose(-1, -2).unsqueeze(-1)  # [B, C, 1, 1]
        return x * y.expand_as(x)

class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.eca = ECABlock(out_channels)  # üîπ –∑–∞–º–µ–Ω–∏–ª–∏ SE –Ω–∞ ECA
        self.act = nn.ReLU(inplace=True)
        self.dropout = nn.Dropout2d(config.dropout)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        residual = self.shortcut(x)
        x = self.act(self.bn1(self.conv1(x)))
        x = self.dropout(x)
        x = self.bn2(self.conv2(x))
        x = self.eca(x)  # üîπ –∏—Å–ø–æ–ª—å–∑—É–µ–º ECA
        return self.act(x + residual)

class AnimeClassifier(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.backbone = nn.Sequential(
            nn.Conv2d(3, 64, 7, stride=2, padding=3, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2, padding=1),
            ResidualBlock(64, 64),
            ResidualBlock(64, 128, stride=2),
            ResidualBlock(128, 256, stride=2),
            ResidualBlock(256, 512, stride=2),
            nn.AdaptiveAvgPool2d(1)
        )
        self.moe = MoE(512, config.num_experts, config.expert_units, config.k_top_expert)
        self.classifier = nn.Sequential(
            nn.Linear(512, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(config.dropout),
            nn.Linear(512, num_classes)
        )

    def forward(self, x):
        x = self.backbone(x).flatten(1)
        x = self.moe(x)
        return self.classifier(x)

# ====================== –û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–• ======================
class ImageDataset(Dataset):
    def __init__(self, root, transform=None, mode='train'):
        # –í—Å–µ–≥–¥–∞ —á–∏—Ç–∞–µ–º –∫–ª–∞—Å—Å—ã –∏–∑ labels.txt
        if os.path.exists(config.labels_path):
            with open(config.labels_path, 'r') as f:
                self.classes = [line.strip() for line in f]
        else:
            # –ï—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ—Ç, –±–µ—Ä–µ–º –∏–∑ –ø–∞–ø–∫–∏ –∏ —Å–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª
            self.classes = sorted(os.listdir(root))
            with open(config.labels_path, 'w') as f:
                f.write('\n'.join(self.classes))

        self.samples = []
        for label, cls in enumerate(self.classes):
            cls_path = os.path.join(root, cls)
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –ø–∞–ø–∫–∞
            if os.path.exists(cls_path):
                self.samples.extend([(f, label) for f in glob(os.path.join(cls_path, '*'))])
            else:
                print(f"‚ö†Ô∏è  –ü–∞–ø–∫–∞ –∫–ª–∞—Å—Å–∞ '{cls}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")

        self.transform = transform or self._get_transforms(mode)

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        img_path, label = self.samples[idx]
        img = np.array(Image.open(img_path).convert('RGB'))  # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ numpy array

        if self.transform:
            augmented = self.transform(image=img)
            img = augmented['image']

        return img, label

    @staticmethod
    def _get_transforms(mode):
        if mode == 'train':
            return A.Compose([
                A.Rotate(limit=30, p=0.5),
                A.RandomResizedCrop(
                    size=config.input_size,
                    scale=(0.8, 1.0),
                    ratio=(0.75, 1.33),
                    interpolation=1,
                    p=1.0
                ),
                A.HorizontalFlip(p=0.5),
                A.ColorJitter(
                    brightness=0.2,
                    contrast=0.2,
                    saturation=0.2,
                    hue=0.1,  # –î–æ–±–∞–≤–∏–ª–∏ hue –¥–ª—è –ª—É—á—à–µ–π –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
                    p=0.5
                ),
                A.GaussianBlur(blur_limit=(3, 3), p=0.2),
                A.Affine(
                    translate_percent=(-0.1, 0.1),
                    keep_ratio=True,
                    p=0.5
                ),
                A.ToFloat(max_value=255.0),
                ToTensorV2(),
            ])
        return A.Compose([
            A.Resize(config.input_size[0], config.input_size[1]),  # –î–æ–±–∞–≤–∏–ª–∏ Resize –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            A.ToFloat(max_value=255.0),
            ToTensorV2(),
        ])

# ====================== –û–ë–£–ß–ï–ù–ò–ï ======================
def mixup_data(x, y, alpha=1.0):
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1
    batch_size = x.size(0)
    index = torch.randperm(batch_size).to(x.device)

    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam

def cutmix_data(x, y, alpha=1.0):
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1
    batch_size, _, h, w = x.size()
    index = torch.randperm(batch_size).to(x.device)

    cx = np.random.randint(w)
    cy = np.random.randint(h)
    cut_w = int(w * np.sqrt(1 - lam))
    cut_h = int(h * np.sqrt(1 - lam))

    x1 = np.clip(cx - cut_w // 2, 0, w)
    x2 = np.clip(cx + cut_w // 2, 0, w)
    y1 = np.clip(cy - cut_h // 2, 0, h)
    y2 = np.clip(cy + cut_h // 2, 0, h)

    x[:, :, y1:y2, x1:x2] = x[index, :, y1:y2, x1:x2]
    y_a, y_b = y, y[index]
    lam = 1 - ((x2 - x1) * (y2 - y1) / (w * h))
    return x, y_a, y_b, lam

def get_class_weights_from_dirs(root_dir, class_names):
    class_counts = []
    for class_name in class_names:
        path = os.path.join(root_dir, class_name)
        count = len(glob(os.path.join(path, "*")))
        class_counts.append(count)

    total = sum(class_counts)
    weights = [total / (count + 1e-6) for count in class_counts]  # –∑–∞—â–∏—Ç–∞ –æ—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0
    weights = torch.tensor(weights)
    weights = weights / weights.max()  # –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    return weights

def focal_loss_with_smoothing(outputs, targets, gamma=5.0, smoothing=0.1, class_weights=None):
    num_classes = outputs.size(1)
    confidence = 1.0 - smoothing

    log_probs = torch.nn.functional.log_softmax(outputs, dim=-1)
    probs = torch.exp(log_probs)

    true_dist = torch.full_like(log_probs, smoothing / (num_classes - 1))
    true_dist.scatter_(1, targets.unsqueeze(1), confidence)

    pt = torch.sum(true_dist * probs, dim=-1)
    focal_factor = (1 - pt).pow(gamma)
    loss = -torch.sum(true_dist * log_probs, dim=-1)

    if class_weights is not None:
        weights = class_weights[targets]
        loss = loss * weights

    return torch.mean(focal_factor * loss)

def forward_with_mixup_cutmix(model, inputs, labels, config, class_weights, device):
    inputs, labels = inputs.to(device), labels.to(device)

    use_mix = np.random.rand() < 0.50
    use_cutmix = np.random.rand() < 0.5

    if use_mix:
        if use_cutmix:
            inputs, targets_a, targets_b, lam = cutmix_data(inputs, labels, alpha=1.0)
        else:
            inputs, targets_a, targets_b, lam = mixup_data(inputs, labels, alpha=1.0)

        with torch.amp.autocast('cuda', enabled=config.mixed_precision):
            outputs = model(inputs)
            loss = lam * focal_loss_with_smoothing(outputs, targets_a, config.focal_gamma, config.smoothing, class_weights)\
                 + (1 - lam) * focal_loss_with_smoothing(outputs, targets_b, config.focal_gamma, config.smoothing, class_weights)
    else:
        with torch.amp.autocast('cuda', enabled=config.mixed_precision):
            outputs = model(inputs)
            loss = focal_loss_with_smoothing(outputs, labels, config.focal_gamma, config.smoothing, class_weights)

    return outputs, loss

def compile_model(model):
    torch.compile(model,
        mode="max-autotune",
        dynamic=False,
        fullgraph=True)
    torch.cuda.empty_cache()

def save_compressed_checkpoint(model, epoch, optimizer, scheduler, path):
    """
    –£–º–Ω–æ–µ —Å–∂–∞—Ç–∏–µ —Å —É—á–µ—Ç–æ–º mixed precision
    """
    # 1. –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫–∏–µ –≤–µ—Å–∞ –º–æ–∂–Ω–æ —Å–∂–∏–º–∞—Ç—å
    checkpoint = {
        'epoch': epoch,
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
    }

    # 2. –°–∂–∏–º–∞–µ–º –í–°–ï –≤–µ—Å–∞ –≤ float16
    compressed_weights = {}
    for name, param in model.state_dict().items():
        if param.is_floating_point():
            compressed_weights[name] = param.half().clone()
        else:
            compressed_weights[name] = param

    checkpoint['model_state_dict'] = compressed_weights

    # 3. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º —Å–∂–∞—Ç–∏–µ–º
    with gzip.open(path, 'wb', compresslevel=9) as f:
        torch.save(checkpoint, f, pickle_protocol=4)

    # 4. –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–∂–∞—Ç–∏—è
    size_mb = os.path.getsize(path) / 1024 / 1024
    temp_path = path + '.tmp'
    torch.save(checkpoint, temp_path)
    uncompressed_size = os.path.getsize(temp_path) / 1024 / 1024
    os.remove(temp_path)

    compression_ratio = (1 - size_mb / uncompressed_size) * 100

    print(f"[System]  –ß–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {size_mb:.1f} MB")
    print(f"[System]  –°–∂–∞—Ç–∏–µ: {compression_ratio:.0f}% –æ—Ç {uncompressed_size:.1f} MB")

    return size_mb

def load_compressed_checkpoint(model, optimizer, scheduler, path, device):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ–º —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
    """
    try:
        # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º
        with gzip.open(path, 'rb') as f:
            checkpoint = torch.load(f, map_location='cpu', weights_only=False)

        # 2. –ü–æ–ª—É—á–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –≤–µ—Å–∞
        saved_weights = checkpoint['model_state_dict']
        current_weights = model.state_dict()
        loaded_weights = {}

        # 3. –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö
        for name in current_weights.keys():
            if name in saved_weights:
                saved_param = saved_weights[name]
                current_param = current_weights[name]

                if saved_param.is_floating_point() and current_param.is_floating_point():
                    loaded_weights[name] = saved_param.to(current_param.dtype)
                else:
                    loaded_weights[name] = saved_param
            else:
                loaded_weights[name] = current_weights[name]
                print(f"‚ö†Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä: {name}")

        # 4. –ó–∞–≥—Ä—É–∂–∞–µ–º –≤ –º–æ–¥–µ–ª—å
        model.load_state_dict(loaded_weights)
        model.to(device)

        # 5. –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º optimizer –∏ scheduler
        if optimizer and 'optimizer_state_dict' in checkpoint and checkpoint['optimizer_state_dict']:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

        if scheduler and 'scheduler_state_dict' in checkpoint and checkpoint['scheduler_state_dict']:
            if hasattr(scheduler, 'load_state_dict'):
                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            else:
                print("‚ö†Ô∏è  Scheduler –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç load_state_dict")

        print(f"‚úì –ß–µ–∫–ø–æ–∏–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω")
        print(f"  –≠–ø–æ—Ö–∞: {checkpoint['epoch']}")

        return {
            'model': model,
            'epoch': checkpoint['epoch'],
            'optimizer': optimizer,
            'scheduler': scheduler,
        }

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
        import traceback
        traceback.print_exc()
        return None

def run_training():
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—á–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
    torch.set_float32_matmul_precision('medium')

    # –í–∫–ª—é—á–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ cuDNN
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    os.makedirs(os.path.dirname(config.checkpoint_path), exist_ok=True)

    full_classes = sorted(os.listdir(config.source_dir))

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º/–ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º —Å–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Å–æ–≤
    with open(config.labels_path, 'w') as f:
        f.write('\n'.join(full_classes))

    print(f"üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(full_classes)}")
    print(f"üöÄ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è:")
    print(f"  ‚Ä¢ Ini LR: {config.ini_lr:.4f}")
    print(f"  ‚Ä¢ Max LR: {config.max_lr:.4f}")
    print(f"  ‚Ä¢ –í—Å–µ–≥–æ —ç–ø–æ—Ö: {config.epochs}")

    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø–æ—Ä—è–¥–∫–æ–º –∫–ª–∞—Å—Å–æ–≤
    full_dataset = ImageDataset(config.source_dir)
    full_dataset.classes = full_classes

    train_size = int((1 - config.val_split) * len(full_dataset))
    train_ds, val_ds = torch.utils.data.random_split(full_dataset, [train_size, len(full_dataset) - train_size])

    train_loader = DataLoader(
        train_ds,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=os.cpu_count()-1,  
        persistent_workers=True,
        prefetch_factor=1,
        pin_memory=True)
    val_loader = DataLoader(
        val_ds,
        batch_size=config.batch_size,
        num_workers=os.cpu_count()-1,
        persistent_workers=True,
        prefetch_factor=1,
        pin_memory=True)

    model = AnimeClassifier(len(full_classes)).to(device)

    class_weights = get_class_weights_from_dirs(config.source_dir, full_classes).to(device)

    optimizer = optim.AdamW(model.parameters(), lr=config.ini_lr)

    scheduler = WarmupReduceLROnPlateau(
        optimizer=optimizer,
        ini_lr=config.ini_lr,
        max_lr=config.max_lr,
        factor=config.plateau_factor,
        threshold=config.plateau_threshold,
    )

    scaler = torch.amp.GradScaler('cuda', enabled=config.mixed_precision and torch.cuda.is_available())
    start_epoch = 1
    early_stop_counter = 0

    if config.resume_training:
        loaded = load_compressed_checkpoint(model, optimizer, scheduler, config.checkpoint_path, device)

        if loaded is not None:
            model = loaded['model']
            optimizer = loaded['optimizer']
            scheduler = loaded['scheduler']
            start_epoch = loaded['epoch'] + 1

            # –ö–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏
            compile_model(model)

            print(f"üîÑ –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å —ç–ø–æ—Ö–∏ {start_epoch}")
            print(f"  –¢–µ–∫—É—â–∏–π LR: {optimizer.param_groups[0]['lr']:.6f}")
        else:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç, –Ω–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è")
            compile_model(model)
            save_compressed_checkpoint(
                model=model,
                epoch=-1,
                optimizer=optimizer,
                scheduler=scheduler,
                path=config.checkpoint_path
            )
            print("[System]  Initial compressed checkpoint saved")
    else:
        compile_model(model)
        save_compressed_checkpoint(
            model=model,
            epoch=-1,
            optimizer=optimizer,
            scheduler=scheduler,
            path=config.checkpoint_path
        )
        print("[System]  Initial compressed checkpoint saved")

    start_time = time.time()
    train_loader_len = len(train_loader)
    val_loader_len = len(val_loader)

    for epoch in range(start_epoch, config.epochs+1):
        model.train()
        train_loss = 0.0
        train_correct, train_total = 0, 0
        optimizer.zero_grad(set_to_none=True)
        epoch_start_time = time.time()

        current_lr = scheduler.get_last_lr()
        print(f"[LR] Current: {current_lr:.8f}")

        for batch_idx, (inputs, labels) in enumerate(train_loader):
            batch_start_time = time.time()
            inputs, labels = inputs.to(device), labels.to(device)

            outputs, loss = forward_with_mixup_cutmix(model, inputs, labels, config, class_weights, device)

            scaler.scale(loss).backward()
            train_loss += loss.item()

            # –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –∫–ª–∏–ø–ø–∏–Ω–≥
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config.gradient_clip)

            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad(set_to_none=True)

            _, predicted = torch.max(outputs, 1)
            current_batch_size = labels.size(0)
            train_total += current_batch_size
            train_correct += (predicted == labels).sum().item()

            batch_duration = time.time() - batch_start_time
            remaining_batches = train_loader_len - (batch_idx + 1)
            estimated_remaining_time = remaining_batches * batch_duration * 3

            remaining_time_str = time.strftime('%H:%M:%S', time.gmtime(estimated_remaining_time))
            print(
                f"\r[Train] Epoch {epoch}/{config.epochs} | Batch {batch_idx+1}/{train_loader_len} | "
                f"Loss: {(loss.item()):.4f} | Remaining time: {remaining_time_str}",
                end='', flush=True)

        train_accuracy = 100 * train_correct / train_total
        print()

        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        model.eval()
        val_loss, val_correct, val_total = 0.0, 0, 0
        all_preds, all_labels = [], []

        with torch.inference_mode(), torch.amp.autocast('cuda', enabled=config.mixed_precision):
            for batch_idx, (inputs, labels) in enumerate(val_loader):
                batch_start_time = time.time()

                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)

                loss = focal_loss_with_smoothing(outputs, labels, config.focal_gamma, config.smoothing)
                val_loss += loss.item()

                _, predicted = torch.max(outputs, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()

                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())

                batch_duration = time.time() - batch_start_time
                remaining_batches = val_loader_len - (batch_idx + 1)
                estimated_remaining_time = remaining_batches * batch_duration * 3
                remaining_time_str = time.strftime('%H:%M:%S', time.gmtime(estimated_remaining_time))

                print(
                    f"\r[Val]   Epoch {epoch}/{config.epochs} | Batch {batch_idx+1}/{val_loader_len} | "
                    f"Loss: {loss.item():.4f} | Remaining: {remaining_time_str}",
                    end='', flush=True)

        print()

        # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
        val_accuracy = 100 * val_correct / val_total
        val_precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)
        val_recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)
        val_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)

        epoch_end_time = time.time()
        epoch_duration = epoch_end_time - epoch_start_time
        total_elapsed_time = epoch_end_time - start_time
        epoch_duration_str = time.strftime("%H:%M:%S", time.gmtime(epoch_duration))
        total_elapsed_str = time.strftime("%H:%M:%S", time.gmtime(total_elapsed_time))

        # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
        if val_loss < scheduler.best_loss:
            early_stop_counter = 0
            save_compressed_checkpoint(
                model=model,
                epoch=epoch,
                optimizer=optimizer,
                scheduler=scheduler,
                path=config.checkpoint_path
            )
            print("[System]  Checkpoint saved (compressed)")
        else:
            early_stop_counter += 1
            if early_stop_counter >= config.early_stopping_patience:
                print(f"[System]  Early Stop (no improvement for {early_stop_counter} epochs)")
                break

        next_lr = scheduler.step(epoch+1, val_loss)

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        print(f"[Summary] Train Loss: {train_loss/len(train_loader):.4f} | Acc: {train_accuracy:.2f}%")
        print(f"[Summary] Val   Loss: {val_loss/len(val_loader):.4f} | Acc: {val_accuracy:.2f}%")
        print(f"[Summary] Val Precision: {val_precision:.4f} | Recall: {val_recall:.4f} | F1: {val_f1:.4f}")
        print(f"[Time]    Epoch: {epoch_duration_str} | Total: {total_elapsed_str}")
        print(f"[LR]      Current: {current_lr:.8f}")
        print(f"[LR]      Next:    {next_lr:.8f}")

        print()

# ====================== –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ======================
def convert_to_onnx():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    with open(config.labels_path) as f:
        classes = [line.strip() for line in f]

    model = AnimeClassifier(len(classes)).to(device)
    optimizer = optim.AdamW(model.parameters(), lr=config.ini_lr)
    scheduler = WarmupReduceLROnPlateau(
        optimizer=optimizer,
        ini_lr=config.ini_lr,
        max_lr=config.max_lr,
        factor=config.plateau_factor,
        threshold=config.plateau_threshold,
    )

    loaded = load_compressed_checkpoint(model, optimizer, scheduler, config.checkpoint_path, device)

    if loaded is None:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ ONNX!")
        return

    model = loaded['model']
    model.eval()

    print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –¥–ª—è ONNX —ç–∫—Å–ø–æ—Ä—Ç–∞")
    print(f"   –ö–ª–∞—Å—Å–æ–≤: {len(classes)}")

    dummy_input = torch.randn(1, 3, *config.input_size).to(device)

    try:
        torch.onnx.export(
            model,
            dummy_input,
            config.onnx_path,
            input_names=['input'],
            output_names=['output'],
            dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}},
            do_constant_folding=True,
            opset_version=14,
            training=torch.onnx.TrainingMode.EVAL,
            verbose=False
        )
        print(f"‚úÖ ONNX –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {config.onnx_path}")

        if os.path.exists(config.onnx_path):
            size_mb = os.path.getsize(config.onnx_path) / 1024 / 1024
            print(f"   –†–∞–∑–º–µ—Ä ONNX —Ñ–∞–π–ª–∞: {size_mb:.2f} MB")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ –≤ ONNX: {e}")
        import traceback
        traceback.print_exc()

def test_onnx():
    if not os.path.exists(config.onnx_path):
        print("‚ùå ONNX –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
        print(f"   –ü—É—Ç—å: {config.onnx_path}")
        return

    try:
        options = ort.SessionOptions()
        options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL

        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if torch.cuda.is_available() else ['CPUExecutionProvider']

        session = ort.InferenceSession(config.onnx_path, options, providers=providers)
        print("‚úÖ ONNX Runtime —Å–µ—Å—Å–∏—è —Å–æ–∑–¥–∞–Ω–∞")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ ONNX –º–æ–¥–µ–ª–∏: {e}")
        return

    test_image_path = os.path.join(config.dir, "test.jpg")
    if not os.path.exists(test_image_path):
        print(f"‚ùå –¢–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {test_image_path}")
        print("   –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª test.jpg –≤ –ø–∞–ø–∫–µ –ø—Ä–æ–µ–∫—Ç–∞")
        return

    try:
        img = Image.open(test_image_path).convert('RGB')
        img_np = np.array(img)

        transform = ImageDataset._get_transforms('val')
        augmented = transform(image=img_np)
        img_tensor = augmented['image'].unsqueeze(0)

        print(f"‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {img.size[0]}x{img.size[1]}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
        return

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    with open(config.labels_path) as f:
        classes = [line.strip() for line in f]

    model = AnimeClassifier(len(classes)).to(device)
    optimizer = optim.AdamW(model.parameters(), lr=config.ini_lr)
    scheduler = WarmupReduceLROnPlateau(
        optimizer=optimizer,
        ini_lr=config.ini_lr,
        max_lr=config.max_lr,
        factor=config.plateau_factor,
        threshold=config.plateau_threshold,
    )

    loaded = load_compressed_checkpoint(model, optimizer, scheduler, config.checkpoint_path, device)

    if loaded is None:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å PyTorch –º–æ–¥–µ–ª—å –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è!")
        return

    model = loaded['model']
    model.eval()

    with torch.no_grad():
        pytorch_output = model(img_tensor.to(device))
        pytorch_probs = torch.softmax(pytorch_output, dim=1).cpu()

    try:
        onnx_input = img_tensor.numpy().astype(np.float32)
        onnx_outputs = session.run(None, {'input': onnx_input})
        onnx_probs = torch.softmax(torch.tensor(onnx_outputs[0]), dim=1)

        print("‚úÖ ONNX inference –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ ONNX inference: {e}")
        return

    print("\n" + "="*50)
    print("[PyTorch] –¢–æ–ø-5 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
    pytorch_top_probs, pytorch_top_indices = torch.topk(pytorch_probs, 5)
    for i, (prob, idx) in enumerate(zip(pytorch_top_probs[0], pytorch_top_indices[0])):
        print(f"{i+1}. {classes[idx]}: {prob.item()*100:.2f}%")

    print("\n[ONNX] –¢–æ–ø-5 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
    onnx_top_probs, onnx_top_indices = torch.topk(onnx_probs, 5)
    for i, (prob, idx) in enumerate(zip(onnx_top_probs[0], onnx_top_indices[0])):
        print(f"{i+1}. {classes[idx]}: {prob.item()*100:.2f}%")

    diff = torch.max(torch.abs(pytorch_probs - onnx_probs)).item()
    print(f"\n[–°—Ä–∞–≤–Ω–µ–Ω–∏–µ] –†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ –º–µ–∂–¥—É –≤—ã—Ö–æ–¥–∞–º–∏: {diff:.6f}")

    if diff < 0.001:
        print("‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞! –†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ < 0.001")
    elif diff < 0.01:
        print("‚ö†Ô∏è  –ù–µ–±–æ–ª—å—à–æ–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ (0.001-0.01), –≤–æ–∑–º–æ–∂–Ω–æ –∏–∑-–∑–∞ —á–∏—Å–ª–µ–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏")
    else:
        print("‚ùå –ë–æ–ª—å—à–æ–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ (> 0.01)! –í–æ–∑–º–æ–∂–Ω–∞—è –æ—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏")

    print("\n" + "="*50)
    print(f"PyTorch device: {device}")
    print(f"PyTorch dtype: {pytorch_probs.dtype}")
    print(f"ONNX dtype: {onnx_probs.dtype}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(classes)}")

# ====================== –ò–ù–¢–ï–†–§–ï–ô–° ======================
def main_menu():
    while True:
        print("\n" + "="*50)
        print("üöÄ Anime Classifier")
        print("="*50)
        print("1. –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å (—Å –Ω—É–ª—è)")
        print("2. –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ")
        print("3. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ ONNX")
        print("4. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å ONNX")
        choice = input("–í—ã–±–æ—Ä: ").strip()

        if choice == '1':
            config.resume_training = False
            run_training()
        elif choice == '2':
            if not os.path.exists(config.checkpoint_path):
                print("‚ùå –ß–µ–∫–ø–æ–∏–Ω—Ç –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω!")
                continue
            config.resume_training = True
            run_training()
        elif choice == '3':
            convert_to_onnx()
        elif choice == '4':
            test_onnx()
        elif choice == '0':
            break
        else:
            print("–ù–µ–≤–µ—Ä–Ω—ã–π –≤–≤–æ–¥!")

if __name__ == "__main__":
    main_menu()