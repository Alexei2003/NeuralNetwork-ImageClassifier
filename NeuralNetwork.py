import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
import os
from glob import glob
from PIL import Image
import onnxruntime as ort
from sklearn.metrics import precision_score, recall_score, f1_score
import time
from torchinfo import summary
import albumentations as A
from albumentations.pytorch import ToTensorV2
import torch.backends.cudnn
import torch.nn.functional as F
import matplotlib.pyplot as plt

# ====================== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ======================
class Config:
    # –≤—ã–±–æ—Ä —Å–∏—Å—Ç–µ–º—ã
    system = "colab"

    match system:
        case "my":
            notebook = False
            dir = "/media/alex/Games/WPS/NeuralNetwork-ImageClassifier/"
        case "colab":
            notebook = True
            dir = "/content/NeuralNetwork-ImageClassifier/"

    # –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º –∏ –º–æ–¥–µ–ª—è–º
    source_dir = dir + "DataSet/ARTS/Original"         # –ü–∞–ø–∫–∞ —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
    checkpoint_path = dir + "Model/best_model.pth"     # –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è/–∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
    labels_path = dir + "Model/labels.txt"             # –§–∞–π–ª —Å –º–µ—Ç–∫–∞–º–∏ –∫–ª–∞—Å—Å–æ–≤
    onnx_path = dir + "Model/model.onnx"               # –ü—É—Ç—å –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –≤ ONNX —Ñ–æ—Ä–º–∞—Ç–µ

    # –§–ª–∞–≥–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏–µ–º
    resume_training = False         # –ü—Ä–æ–¥–æ–ª–∂–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞, –µ—Å–ª–∏ True

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    input_size = (224, 224)         # –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (—à–∏—Ä–∏–Ω–∞, –≤—ã—Å–æ—Ç–∞)

    # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ –∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    num_experts = 16                # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –≤ MoE (Mixture of Experts)
    expert_units = 1024             # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ –∫–∞–∂–¥–æ–º —ç–∫—Å–ø–µ—Ä—Ç–µ
    k_top_expert = 4                # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –Ω–∞ –æ–¥–∏–Ω –ø—Ä–∏–º–µ—Ä
    se_reduction = 16               # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Ä–µ–¥—É–∫—Ü–∏–∏ –¥–ª—è SE (Squeeze-and-Excitation) –±–ª–æ–∫–∞
    dropout = 0.5                   # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—Ç–∫–ª—é—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–æ–≤ (dropout)

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    lr = 0.001                       # –ù–∞—á–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (learning rate)
    batch_size = 256                # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (—á–∏—Å–ª–æ –ø—Ä–∏–º–µ—Ä–æ–≤, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã—Ö –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥)
    epochs = 100                    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è (–ø–æ–ª–Ω—ã—Ö –ø—Ä–æ—Ö–æ–¥–æ–≤ –ø–æ –≤—Å–µ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É)
    focal_gamma = 2                 # –ü–∞—Ä–∞–º–µ—Ç—Ä –≥–∞–º–º–∞ –¥–ª—è Focal Loss, —Ä–µ–≥—É–ª–∏—Ä—É–µ—Ç —Å—Ç–µ–ø–µ–Ω—å —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–∏ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö
    smoothing = 0.1                 # –ü–∞—Ä–∞–º–µ—Ç—Ä label smoothing, –∑–∞–¥–∞—ë—Ç —É—Ä–æ–≤–µ–Ω—å —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è –º–µ—Ç–æ–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±–æ–±—â–µ–Ω–∏—è
    threshold = 1e-2                # –†–∞–∑–Ω–∏—Ü–∞ val loss –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è learning rate

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ –∫–æ–Ω—Ç—Ä–æ–ª—è –æ–±—É—á–µ–Ω–∏—è
    mixed_precision = True          # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–º–µ—à–∞–Ω–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å (fp16) –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
    early_stopping_patience = 5     # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è –¥–ª—è —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
    val_split = 0.2                 # –î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö, –≤—ã–¥–µ–ª—è–µ–º–∞—è –ø–æ–¥ –≤–∞–ª–∏–¥–∞—Ü–∏—é
    factor_lr = 0.5                 # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —É–º–µ–Ω—å—à–µ–Ω–∏—è learning rate –ø—Ä–∏ plateau
    patience_lr = 0                 # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è learning rate

config = Config()

# ====================== –ö–û–ú–ü–û–ù–ï–ù–¢–´ –ú–û–î–ï–õ–ò ======================
class MoE(nn.Module):
    def __init__(self, input_dim, num_experts, expert_units, k_top):
        super().__init__()
        self.num_experts = num_experts
        self.k_top = k_top
        self.experts = nn.ModuleList([
            nn.Sequential(
                nn.Linear(input_dim, expert_units),
                nn.ReLU(inplace=True) ,
                nn.Dropout(config.dropout),
                nn.Linear(expert_units, input_dim))
            for _ in range(num_experts)
        ])
        self.router = nn.Linear(input_dim, num_experts)

    def forward(self, x):
        logits = self.router(x)
        top_k_weights, top_k_indices = logits.topk(self.k_top, dim=1)
        top_k_weights = torch.softmax(top_k_weights, dim=1)

        # –°–æ–∑–¥–∞–µ–º —Ç–µ–Ω–∑–æ—Ä –≤—Å–µ—Ö —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã—Ö –≤—ã—Ö–æ–¥–æ–≤
        expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=1)  # [B, num_experts, D]

        # –°–æ–∑–¥–∞–µ–º –º–∞—Å–∫—É –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ [B, num_experts, D]
        mask = torch.zeros_like(expert_outputs)
        mask = torch.scatter(
            mask,
            1,
            top_k_indices.unsqueeze(-1).expand(-1, -1, expert_outputs.size(-1)),
            1.0
        )

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Ç–æ–ª—å–∫–æ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤
        expert_outputs = expert_outputs * mask + (expert_outputs * (1 - mask)).detach()

        # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ø-k —ç–∫—Å–ø–µ—Ä—Ç–æ–≤
        selected_outputs = expert_outputs.gather(
            1,
            top_k_indices.unsqueeze(-1).expand(-1, -1, expert_outputs.size(-1))
        )

        # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ
        output = (selected_outputs * top_k_weights.unsqueeze(-1)).sum(dim=1)
        return output + x

class ECABlock(nn.Module):
    def __init__(self, channels, k_size=3):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        # 1D —Å–≤—ë—Ä—Ç–∫–∞ –ø–æ –∫–∞–Ω–∞–ª–∞–º
        self.conv = nn.Conv1d(1, 1, kernel_size=k_size,
                              padding=(k_size - 1) // 2, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        y = self.avg_pool(x)  # [B, C, 1, 1]
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º—É [B, 1, C] –¥–ª—è 1D conv
        y = self.conv(y.squeeze(-1).transpose(-1, -2))
        y = self.sigmoid(y).transpose(-1, -2).unsqueeze(-1)  # [B, C, 1, 1]
        return x * y.expand_as(x)


class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.eca = ECABlock(out_channels)  # üîπ –∑–∞–º–µ–Ω–∏–ª–∏ SE –Ω–∞ ECA
        self.act = nn.ReLU(inplace=True)
        self.dropout = nn.Dropout2d(config.dropout)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        residual = self.shortcut(x)
        x = self.act(self.bn1(self.conv1(x)))
        x = self.dropout(x)
        x = self.bn2(self.conv2(x))
        x = self.eca(x)  # üîπ –∏—Å–ø–æ–ª—å–∑—É–µ–º ECA
        return self.act(x + residual)

class AnimeClassifier(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.backbone = nn.Sequential(
            nn.Conv2d(3, 64, 7, stride=2, padding=3, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2, padding=1),
            ResidualBlock(64, 64),
            ResidualBlock(64, 128, stride=2),
            ResidualBlock(128, 256, stride=2),
            ResidualBlock(256, 512, stride=2),
            nn.AdaptiveAvgPool2d(1)
        )
        self.moe = MoE(512, config.num_experts, config.expert_units, config.k_top_expert)
        self.classifier = nn.Sequential(
            nn.Linear(512, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(config.dropout),
            nn.Linear(512, num_classes)
        )

    def forward(self, x):
        x = self.backbone(x).flatten(1)
        x = self.moe(x)
        return self.classifier(x)

# ====================== –û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–• ======================
class ImageDataset(Dataset):
    def __init__(self, root, transform=None, mode='train'):
        if os.path.exists(config.labels_path):
            with open(config.labels_path, 'r') as f:
                self.classes = [line.strip() for line in f]
        else:
            self.classes = sorted(os.listdir(root))

        self.samples = []
        for label, cls in enumerate(self.classes):
            cls_path = os.path.join(root, cls)
            self.samples.extend([(f, label) for f in glob(os.path.join(cls_path, '*'))])

        self.transform = transform or self._get_transforms(mode)

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        img_path, label = self.samples[idx]
        img = np.array(Image.open(img_path).convert('RGB'))  # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ numpy array

        if self.transform:
            augmented = self.transform(image=img)
            img = augmented['image']

        return img, label

    @staticmethod
    def _get_transforms(mode):
        if mode == 'train':
            return A.Compose([
                A.Rotate(limit=30, p=0.5),
                A.RandomResizedCrop(
                    size=config.input_size,
                    scale=(0.8, 1.0),
                    ratio=(0.75, 1.33),          # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (0.75, 1.33))
                    interpolation=1,             # BILINEAR
                    p=1.0
                ),
                A.HorizontalFlip(p=0.5),
                A.ColorJitter(
                    brightness=0.2,
                    contrast=0.2,
                    saturation=0.2,
                    hue=0.0,                     # –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä
                    p=0.5
                ),
                A.GaussianBlur(blur_limit=(3, 3), p=0.2),
                A.Affine(
                    translate_percent=(-0.1, 0.1),
                    keep_ratio=True,
                    p=0.5
                ),
                A.ToFloat(max_value=255.0),
                ToTensorV2(),
            ])
        return A.Compose([
            A.ToFloat(max_value=255.0),
            ToTensorV2(),
        ])

# ====================== –û–ë–£–ß–ï–ù–ò–ï ======================
def mixup_data(x, y, alpha=1.0):
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1
    batch_size = x.size(0)
    index = torch.randperm(batch_size).to(x.device)

    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam

def cutmix_data(x, y, alpha=1.0):
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1
    batch_size, _, h, w = x.size()
    index = torch.randperm(batch_size).to(x.device)

    cx = np.random.randint(w)
    cy = np.random.randint(h)
    cut_w = int(w * np.sqrt(1 - lam))
    cut_h = int(h * np.sqrt(1 - lam))

    x1 = np.clip(cx - cut_w // 2, 0, w)
    x2 = np.clip(cx + cut_w // 2, 0, w)
    y1 = np.clip(cy - cut_h // 2, 0, h)
    y2 = np.clip(cy + cut_h // 2, 0, h)

    x[:, :, y1:y2, x1:x2] = x[index, :, y1:y2, x1:x2]
    y_a, y_b = y, y[index]
    lam = 1 - ((x2 - x1) * (y2 - y1) / (w * h))
    return x, y_a, y_b, lam

def get_class_weights_from_dirs(root_dir, class_names):
    class_counts = []
    for class_name in class_names:
        path = os.path.join(root_dir, class_name)
        count = len(glob(os.path.join(path, "*")))
        class_counts.append(count)

    total = sum(class_counts)
    weights = [total / (count + 1e-6) for count in class_counts]  # –∑–∞—â–∏—Ç–∞ –æ—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0
    weights = torch.tensor(weights)
    weights = weights / weights.mean()  # –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    return weights

def focal_loss_with_smoothing(outputs, targets, gamma=5.0, smoothing=0.1, class_weights=None):
    num_classes = outputs.size(1)
    confidence = 1.0 - smoothing

    log_probs = torch.nn.functional.log_softmax(outputs, dim=-1)
    probs = torch.exp(log_probs)

    true_dist = torch.full_like(log_probs, smoothing / (num_classes - 1))
    true_dist.scatter_(1, targets.unsqueeze(1), confidence)

    pt = torch.sum(true_dist * probs, dim=-1)
    focal_factor = (1 - pt).pow(gamma)
    loss = -torch.sum(true_dist * log_probs, dim=-1)

    if class_weights is not None:
        weights = class_weights[targets]
        loss = loss * weights

    return torch.mean(focal_factor * loss)

def imshow(img_tensor, title=None):
    # img_tensor: Tensor —Å —Ñ–æ—Ä–º–∞—Ç–æ–º (C, H, W)
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–Ω–∑–æ—Ä –≤ numpy –¥–ª—è matplotlib –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ [0,1]
    img = img_tensor.cpu().numpy()
    img = np.transpose(img, (1, 2, 0))  # C,H,W -> H,W,C
    img = np.clip(img, 0, 1)  # –ß—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º —Å —Ü–≤–µ—Ç–∞–º–∏

    plt.imshow(img)
    if title:
        plt.title(title)
    plt.axis('off')
    plt.savefig('output_image.png')
    plt.close()

def forward_with_mixup_cutmix(model, inputs, labels, config, class_weights, device):
    inputs, labels = inputs.to(device), labels.to(device)

    use_mix = np.random.rand() < 0.50
    use_cutmix = np.random.rand() < 0.5

    if use_mix:
        if use_cutmix:
            inputs, targets_a, targets_b, lam = cutmix_data(inputs, labels, alpha=1.0)
        else:
            inputs, targets_a, targets_b, lam = mixup_data(inputs, labels, alpha=1.0)

        #imshow(inputs[1], title="Original image")

        with torch.amp.autocast('cuda', enabled=config.mixed_precision):
            outputs = model(inputs)
            loss = lam * focal_loss_with_smoothing(outputs, targets_a, config.focal_gamma, config.smoothing, class_weights)\
                 + (1 - lam) * focal_loss_with_smoothing(outputs, targets_b, config.focal_gamma, config.smoothing, class_weights)
    else:
        with torch.amp.autocast('cuda', enabled=config.mixed_precision):
            outputs = model(inputs)
            loss = focal_loss_with_smoothing(outputs, labels, config.focal_gamma, config.smoothing, class_weights)

    return outputs, loss

def compile_model(model):
    torch.compile(model,
        mode="max-autotune",
        dynamic=False,
        fullgraph=False)
    torch.cuda.empty_cache()

def run_training():
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—á–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π (–ù–û–í–û–ï)
    torch.set_float32_matmul_precision('medium')

    # –í–∫–ª—é—á–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ cuDNN
    torch.backends.cudnn.benchmark = True

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    os.makedirs(os.path.dirname(config.checkpoint_path), exist_ok=True)

    if config.resume_training and os.path.exists(config.labels_path):
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–∞—Ä—ã–µ –∫–ª–∞—Å—Å—ã –∏ –¥–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –≤ –∫–æ–Ω–µ—Ü
        with open(config.labels_path, 'r') as f:
            old_classes = [line.strip() for line in f]
        current_classes = sorted(os.listdir(config.source_dir))
        new_classes = [cls for cls in current_classes if cls not in old_classes]
        full_classes = old_classes + new_classes
        if new_classes:
            print(f"–î–æ–±–∞–≤–ª–µ–Ω—ã –Ω–æ–≤—ã–µ –∫–ª–∞—Å—Å—ã –≤ —Ñ–∞–π–ª: {', '.join(new_classes)}")
        else:
            print("–ù–æ–≤—ã—Ö –∫–ª–∞—Å—Å–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
    else:
        full_classes = sorted(os.listdir(config.source_dir))

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Å–æ–≤
    with open(config.labels_path, 'w') as f:
        f.write('\n'.join(full_classes))

    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø–æ—Ä—è–¥–∫–æ–º –∫–ª–∞—Å—Å–æ–≤
    full_dataset = ImageDataset(config.source_dir)
    full_dataset.classes = full_classes  # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Ä—è–¥–æ–∫

    train_size = int((1 - config.val_split) * len(full_dataset))
    train_ds, val_ds = torch.utils.data.random_split(full_dataset, [train_size, len(full_dataset) - train_size])

    train_loader = DataLoader(
        train_ds,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=os.cpu_count(),
        persistent_workers=True,
        prefetch_factor=2,
        pin_memory=True)
    val_loader = DataLoader(
        val_ds,
        batch_size=config.batch_size,
        num_workers=os.cpu_count(),
        persistent_workers=True,
        prefetch_factor=2,
        pin_memory=True)

    model = AnimeClassifier(len(full_classes)).to(device)

    class_weights = get_class_weights_from_dirs(config.source_dir, full_classes).to(device)

    optimizer = optim.AdamW(model.parameters(), lr=config.lr)
    plateau_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer,
        mode='min',
        factor=config.factor_lr,
        patience=config.patience_lr,
        threshold=config.threshold, # ‚Üê –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
        threshold_mode='rel',       # ‚Üê –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
    )
    scaler = torch.amp.GradScaler('cuda', enabled=config.mixed_precision and torch.cuda.is_available())
    start_epoch = 0
    best_loss = float('inf')
    early_stop_counter = 0

    if config.resume_training:
        checkpoint = torch.load(config.checkpoint_path)

        # –£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–µ—Ñ–∏–∫—Å–∞ _orig_mod. –∏–∑ –∫–ª—é—á–µ–π (–µ—Å–ª–∏ –µ—Å—Ç—å)
        checkpoint['model_state_dict'] = {
            k.replace("_orig_mod.", ""): v
            for k, v in checkpoint['model_state_dict'].items()
        }

        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∫–ª—é—á–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ '.weight'
        classifier_keys = [k for k in checkpoint['model_state_dict'] if k.startswith('classifier') and '.weight' in k]
        # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ—Ç, —á—Ç–æ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –∏–Ω–¥–µ–∫—Å–æ–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'classifier.3.weight')
        classifier_weight_key = sorted(classifier_keys, key=lambda k: int(k.split('.')[1]))[-1]
        if not classifier_weight_key:
            raise KeyError("‚ùå –ö–ª—é—á –¥–ª—è –≤–µ—Å–æ–≤ classifier —Å–ª–æ—è –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —á–µ–∫–ø–æ–∏–Ω—Ç–µ!")
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤
        saved_num_classes = checkpoint['model_state_dict'][classifier_weight_key].shape[0]
        current_num_classes = len(full_dataset.classes)

        # –ß–∞—Å—Ç–∏—á–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤ (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º classifier —Å–ª–æ–π)
        model.load_state_dict(
            {k: v for k, v in checkpoint['model_state_dict'].items()
            if not ('classifier.3' in k and ('weight' in k or 'bias' in k))},
            strict=False
        )

        # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö –≤–µ—Å–æ–≤ classifier —Å–ª–æ—è
        with torch.no_grad():
            model.classifier[3].weight.data[:saved_num_classes] = checkpoint['model_state_dict']['classifier.3.weight'][:saved_num_classes]
            model.classifier[3].bias.data[:saved_num_classes] = checkpoint['model_state_dict']['classifier.3.bias'][:saved_num_classes]

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–æ–≤—ã—Ö –≤–µ—Å–æ–≤
        if current_num_classes > saved_num_classes:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–æ–≤—ã—Ö –≤–µ—Å–æ–≤
            with torch.no_grad():
                nn.init.kaiming_normal_(
                    model.classifier[3].weight.data[saved_num_classes:],
                    mode='fan_out',
                    nonlinearity='linear'
                )
                nn.init.constant_(
                    model.classifier[3].bias.data[saved_num_classes:],
                    0.0
                )

            print(f"üÜï –î–æ–±–∞–≤–ª–µ–Ω–æ {current_num_classes - saved_num_classes} –Ω–æ–≤—ã—Ö –∫–ª–∞—Å—Å–æ–≤")

            # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Å –Ω–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            optimizer = optim.AdamW(model.parameters(), lr=config.lr)

            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å—Ç–∞—Ä–æ–≥–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
            old_optimizer_state = checkpoint['optimizer_state_dict']

            # –°–æ–∑–¥–∞–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            new_optimizer_state = {
                'state': {},  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å –Ω—É–ª—è
                'param_groups': old_optimizer_state['param_groups']  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≥—Ä—É–ø–ø
            }

            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            for param_name, param_state in old_optimizer_state['state'].items():
                if param_name in optimizer.state_dict()['state']:
                    new_optimizer_state['state'][param_name] = param_state

            optimizer.load_state_dict(new_optimizer_state)

            # –ö–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏
            compile_model(model)

            # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π
            start_epoch = checkpoint['epoch'] + 1
            best_loss = checkpoint['best_loss']
            early_stop_counter = checkpoint['early_stop_counter']

            print(f"üîÑ –°—Ç–∞—Ä—Ç –¥–æ–æ–±—É—á–µ–Ω–∏—è, –Ω–æ–≤—ã—Ö –∫–ª–∞—Å—Å–æ–≤: {current_num_classes - saved_num_classes}")
        else:
            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
            compile_model(model)

            #4. –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏–π
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            plateau_scheduler.load_state_dict(checkpoint['scheduler_plateau'])
            scaler.load_state_dict(checkpoint['scaler_state_dict'])
            start_epoch = checkpoint['epoch'] + 1
            best_loss = checkpoint['best_loss']
            early_stop_counter = checkpoint['early_stop_counter']
            print(f"üîÑ –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å —ç–ø–æ—Ö–∏ {start_epoch}")
    else:
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        compile_model(model)

        torch.save({
            'epoch': -1,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_plateau': plateau_scheduler.state_dict(),
            'scaler_state_dict': scaler.state_dict(),
            'best_loss': best_loss,
            'early_stop_counter': early_stop_counter,
        }, config.checkpoint_path)

    summary(model, input_size=(1, 3, 224, 224))

    start_time = time.time()  # –ó–∞—Å–µ–∫–∞–µ–º –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞
    optimizer.zero_grad(set_to_none=True)  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
    train_loader_len = len(train_loader)
    val_loader_len = len(val_loader)
    for epoch in range(start_epoch, config.epochs):
        model.train()
        train_loss = 0.0
        train_correct, train_total = 0, 0
        optimizer.zero_grad()

        epoch_start_time = time.time()  # –í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ —ç–ø–æ—Ö–∏
        for batch_idx, (inputs, labels) in enumerate(train_loader):
            batch_start_time = time.time()  # –í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–∞
            inputs, labels = inputs.to(device), labels.to(device)

            outputs, loss = forward_with_mixup_cutmix(model, inputs, labels, config, class_weights, device)
            # –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ (–æ—Å–Ω–æ–≤–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ)
            scaler.scale(loss).backward()
            train_loss += loss.item()

            # –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –∫–ª–∏–ø–ø–∏–Ω–≥
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            # –®–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad(set_to_none=True)

            # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏
            _, predicted = torch.max(outputs, 1)
            current_batch_size = labels.size(0)
            train_total += current_batch_size
            train_correct += (predicted == labels).sum().item()

            # –†–∞—Å—á–µ—Ç –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            batch_duration = time.time() - batch_start_time
            remaining_batches = train_loader_len - (batch_idx + 1)
            estimated_remaining_time = remaining_batches * batch_duration * 3

            remaining_time_str = time.strftime('%H:%M:%S', time.gmtime(estimated_remaining_time))
            print(
                f"\r[Train] Epoch {epoch+1}/{config.epochs} | Batch {batch_idx+1}/{train_loader_len} | "
                f"Loss: {(loss.item()):.4f} | Remaining time: {remaining_time_str}",
                end='', flush=True)

        train_accuracy = 100 * train_correct / train_total
        print()

        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        model.eval()
        val_loss, val_correct, val_total = 0.0, 0, 0
        all_preds, all_labels = [], []

        with torch.inference_mode(), torch.amp.autocast('cuda', enabled=config.mixed_precision):
            for batch_idx, (inputs, labels) in enumerate(val_loader):
                batch_start_time = time.time()

                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)

                loss = focal_loss_with_smoothing(outputs, labels, config.focal_gamma, config.smoothing)
                val_loss += loss.item()

                _, predicted = torch.max(outputs, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()

                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())

                # –†–∞—Å—á–µ—Ç –æ—Å—Ç–∞–≤—à–µ–≥–æ—Å—è –≤—Ä–µ–º–µ–Ω–∏
                batch_duration = time.time() - batch_start_time
                remaining_batches = val_loader_len - (batch_idx + 1)
                estimated_remaining_time = remaining_batches * batch_duration * 3
                remaining_time_str = time.strftime('%H:%M:%S', time.gmtime(estimated_remaining_time))

                print(
                    f"\r[Val]   Epoch {epoch+1}/{config.epochs} | Batch {batch_idx+1}/{val_loader_len} | "
                    f"Loss: {loss.item():.4f} | Remaining: {remaining_time_str}",
                    end='', flush=True)

        print()

        current_lr = optimizer.param_groups[0]['lr']
        # –£–º–µ–Ω—å—à–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è
        plateau_scheduler.step(val_loss)
        next_lr = optimizer.param_groups[0]['lr']

        # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
        val_accuracy = 100 * val_correct / val_total
        val_precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)
        val_recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)
        val_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)

        epoch_end_time = time.time()  # –í—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è —ç–ø–æ—Ö–∏
        epoch_duration = epoch_end_time - epoch_start_time
        total_elapsed_time = epoch_end_time - start_time
        epoch_duration_str = time.strftime("%H:%M:%S", time.gmtime(epoch_duration))
        total_elapsed_str = time.strftime("%H:%M:%S", time.gmtime(total_elapsed_time))

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        print(f"[Summary] Train Loss: {train_loss/len(train_loader):.4f} | Acc: {train_accuracy:.2f}%")
        print(f"[Summary] Val   Loss: {val_loss/len(val_loader):.4f} | Acc: {val_accuracy:.2f}%")
        print(f"[Summary] Val Precision: {val_precision:.4f} | Recall: {val_recall:.4f} | F1: {val_f1:.4f}")
        print(f"[Time]    Epoch: {epoch_duration_str} | Total: {total_elapsed_str}")
        print(f"[Summary] LR: {current_lr:.10f}")
        print(f"[Summary] Next LR: {next_lr:.10f}")

        # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
        if val_loss < best_loss:
            best_loss = val_loss
            early_stop_counter = 0
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_plateau': plateau_scheduler.state_dict(),
                'scaler_state_dict': scaler.state_dict(),
                'best_loss': best_loss,
                'early_stop_counter': early_stop_counter,
            }, config.checkpoint_path)
            print("[System]  Save Checkpoint")
        else:
            early_stop_counter += 1
            if early_stop_counter >= config.early_stopping_patience:
                print("[System]  Early Stop")
                break
        print()

# ====================== –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ======================
def convert_to_onnx():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = AnimeClassifier(len(get_classes())).to(device)
    checkpoint = torch.load(config.checkpoint_path)

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
    model_state_dict = checkpoint['model_state_dict']

    # –£–¥–∞–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å _orig_mod. –∏–∑ –∫–ª—é—á–µ–π (–µ—Å–ª–∏ –µ—Å—Ç—å)
    model_state_dict = {
        k.replace("_orig_mod.", ""): v
        for k, v in model_state_dict.items()
    }

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
    model.load_state_dict(model_state_dict)
    model.eval()

    # –≠–∫—Å–ø–æ—Ä—Ç –≤ ONNX
    dummy_input = torch.randn(1, 3, *config.input_size).to(device)
    torch.onnx.export(
        model,
        dummy_input,
        config.onnx_path,
        input_names=['input'],
        output_names=['output'],
        dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}},
        opset_version=13,
        do_constant_folding=True,
        training=torch.onnx.TrainingMode.EVAL,
        operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK
    )
    print("‚úÖ ONNX –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞:", config.onnx_path)

def test_onnx():
    if not os.path.exists(config.onnx_path):
        print("ONNX –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
        return

    # –ó–∞–≥—Ä—É–∑–∫–∞ ONNX-–º–æ–¥–µ–ª–∏
    session = ort.InferenceSession(config.onnx_path)

    try:
        img = Image.open(config.dir +"NeuralNetwork-ImageClassifier//test.jpg").convert('RGB')
        img_np = np.array(img)

        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π
        transform = ImageDataset._get_transforms('val')
        augmented = transform(image=img_np)
        img_tensor = augmented['image'].unsqueeze(0)
    except FileNotFoundError:
        print("–§–∞–π–ª test.jpg –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return

    # ====================== PyTorch –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ ======================
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –∑–∞–≥—Ä—É–∑–∫–∞ PyTorch-–º–æ–¥–µ–ª–∏
    model = AnimeClassifier(len(get_classes())).to(device)
    checkpoint = torch.load(config.checkpoint_path)

    # –£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤ _orig_mod. (–µ—Å–ª–∏ –µ—Å—Ç—å)
    model_state_dict = {
        k.replace("_orig_mod.", ""): v
        for k, v in checkpoint['model_state_dict'].items()
    }
    model.load_state_dict(model_state_dict)
    model.eval()

    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ PyTorch
    with torch.no_grad():
        pytorch_output = model(img_tensor.to(device))
        pytorch_probs = torch.softmax(pytorch_output, dim=1).cpu()

    # ====================== ONNX –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ ======================
    onnx_outputs = session.run(None, {'input': img_tensor.numpy().astype(np.float32)})
    onnx_probs = torch.softmax(torch.tensor(onnx_outputs[0]), dim=1)

    # ====================== –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ======================
    with open(config.labels_path) as f:
        classes = [line.strip() for line in f]

    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã PyTorch
    print("\n[PyTorch] –¢–æ–ø-5 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
    pytorch_top_probs, pytorch_top_indices = torch.topk(pytorch_probs, 5)
    for i, (prob, idx) in enumerate(zip(pytorch_top_probs[0], pytorch_top_indices[0])):
        print(f"{i+1}. {classes[idx]}: {prob.item()*100:.2f}%")

    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã ONNX
    print("\n[ONNX] –¢–æ–ø-5 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
    onnx_top_probs, onnx_top_indices = torch.topk(onnx_probs, 5)
    for i, (prob, idx) in enumerate(zip(onnx_top_probs[0], onnx_top_indices[0])):
        print(f"{i+1}. {classes[idx]}: {prob.item()*100:.2f}%")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    diff = torch.max(torch.abs(pytorch_probs - onnx_probs)).item()
    print(f"\n–†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ –º–µ–∂–¥—É –≤—ã—Ö–æ–¥–∞–º–∏: {diff:.6f}")
    if diff > 0.001:
        print("‚ö†Ô∏è –í–æ–∑–º–æ–∂–Ω–∞—è –æ—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏! –†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ > 0.001")

def get_classes():
    with open(config.labels_path) as f:
        return [line.strip() for line in f]

# ====================== –ò–ù–¢–ï–†–§–ï–ô–° ======================
def main_menu():
    while True:
        print("\n–ú–µ–Ω—é:")
        print("1. –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å")
        print("2. –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ")  # –ù–æ–≤–∞—è –æ–ø—Ü–∏—è
        print("3. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ ONNX")
        print("4. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å ONNX")
        print("0. –í—ã—Ö–æ–¥")
        choice = input("–í—ã–±–æ—Ä: ").strip()

        if choice == '1':
            config.resume_training = False
            run_training()
        elif choice == '2':
            if not os.path.exists(config.checkpoint_path):
                print("‚ùå –ß–µ–∫–ø–æ–∏–Ω—Ç –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω!")
                continue
            config.resume_training = True
            run_training()
        elif choice == '3':
            convert_to_onnx()
        elif choice == '4':
            test_onnx()
        elif choice == '0':
            break
        else:
            print("–ù–µ–≤–µ—Ä–Ω—ã–π –≤–≤–æ–¥!")

if __name__ == "__main__":
    main_menu()
