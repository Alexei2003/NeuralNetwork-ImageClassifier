import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
import os
from glob import glob
from PIL import Image
import onnxruntime as ort
from sklearn.metrics import precision_score, recall_score, f1_score
import time
import albumentations as A
from albumentations.pytorch import ToTensorV2
import torch.backends.cudnn
from IPython.display import Audio, display

# ====================== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ======================
class Config:
    # –≤—ã–±–æ—Ä —Å–∏—Å—Ç–µ–º—ã
    dir_data = "/content/NeuralNetwork-ImageClassifier/DataSet/ARTS/"
    dir_save = "/content/drive/MyDrive/Colab Notebooks/NeuralNetwork-ImageClassifier/Model/"

    # –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º –∏ –º–æ–¥–µ–ª—è–º
    source_dir = dir_data + "Original"                     # –ü–∞–ø–∫–∞ —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
    checkpoint_path = dir_save + "best_model.pth"     # –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è/–∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
    labels_path = dir_save + "labels.txt"             # –§–∞–π–ª —Å –º–µ—Ç–∫–∞–º–∏ –∫–ª–∞—Å—Å–æ–≤
    onnx_path = dir_save + "model.onnx"               # –ü—É—Ç—å –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –≤ ONNX —Ñ–æ—Ä–º–∞—Ç–µ

    # –§–ª–∞–≥–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏–µ–º
    resume_training = False         # –ü—Ä–æ–¥–æ–ª–∂–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞, –µ—Å–ª–∏ True

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    input_size = (224, 224)         # –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (—à–∏—Ä–∏–Ω–∞, –≤—ã—Å–æ—Ç–∞)

    # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ –∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    num_experts = 16                # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –≤ MoE (Mixture of Experts)
    expert_units = 1024             # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ –∫–∞–∂–¥–æ–º —ç–∫—Å–ø–µ—Ä—Ç–µ
    k_top_expert = 4                # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –Ω–∞ –æ–¥–∏–Ω –ø—Ä–∏–º–µ—Ä
    se_reduction = 16               # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Ä–µ–¥—É–∫—Ü–∏–∏ –¥–ª—è SE (Squeeze-and-Excitation) –±–ª–æ–∫–∞
    dropout = 0.5                   # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—Ç–∫–ª—é—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–æ–≤ (dropout)

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    val_split = 0.2                 # –î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö, –≤—ã–¥–µ–ª—è–µ–º–∞—è –ø–æ–¥ –≤–∞–ª–∏–¥–∞—Ü–∏—é
    gradient_clip = 1.0             # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞
    batch_size = 256                # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (—á–∏—Å–ª–æ –ø—Ä–∏–º–µ—Ä–æ–≤, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã—Ö –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥)
    epochs = 100                    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è (–ø–æ–ª–Ω—ã—Ö –ø—Ä–æ—Ö–æ–¥–æ–≤ –ø–æ –≤—Å–µ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É)
    focal_gamma = 3                 # –ü–∞—Ä–∞–º–µ—Ç—Ä –≥–∞–º–º–∞ –¥–ª—è Focal Loss, —Ä–µ–≥—É–ª–∏—Ä—É–µ—Ç —Å—Ç–µ–ø–µ–Ω—å —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–∏ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö
    smoothing = 0.1                 # –ü–∞—Ä–∞–º–µ—Ç—Ä label smoothing, –∑–∞–¥–∞—ë—Ç —É—Ä–æ–≤–µ–Ω—å —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è –º–µ—Ç–æ–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±–æ–±—â–µ–Ω–∏—è
    mixed_precision = True          # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–º–µ—à–∞–Ω–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å (fp16) –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã LR
    max_lr = 0.001                  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (learning rate)
    ini_lr = 0.0001                 # –ù–∞—á–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
    plateau_factor = 0.8            # –£–º–µ–Ω—å—à–∞—Ç—å lr
    plateau_factor_threshold = 0.9  # –£–º–µ–Ω—å—à–∞—Ç—å threshold
    plateau_threshold = 0.05        # –ü–æ—Ä–æ–≥ —É–ª—É—á—à–µ–Ω–∏—è (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π)
    early_stopping_patience = 10    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è –¥–ª—è —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏

config = Config()

# ====================== –ö–û–°–ò–ù–£–°–ù–´–ô –®–ï–î–£–õ–ï–† –° WARMUP ======================
class WarmupReduceLROnPlateau():
    """Warmup + ReduceLROnPlateau –ª–æ–≥–∏–∫–∞"""

    def __init__(self, optimizer, ini_lr, max_lr, factor, factor_threshold, threshold):
        self.optimizer = optimizer

        self.max_lr = max_lr
        self.ini_lr = ini_lr
        self.current_epoch = 0

        # ReduceLROnPlateau –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.factor = factor
        self.threshold = threshold
        self.factor_threshold = factor_threshold
        self.num_bad = 0
        self.speed = 1

        # –¢—Ä–µ–∫–∏–Ω–≥ –ª—É—á—à–µ–≥–æ loss
        self.best_loss = float('inf')

    def step(self, epoch=None, validation_loss=None):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Ü–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏ —Å validation_loss"""
        if epoch is not None:
            self.current_epoch = epoch
        else:
            self.current_epoch += 1

        # Warmup —Ñ–∞–∑–∞
        if self.current_epoch < 3:
            if self.current_epoch == 1:
                lr = self.ini_lr
            else:
                lr = self.max_lr

            for param_group in self.optimizer.param_groups:
                param_group['lr'] = lr

            self.best_loss = validation_loss
            return self.optimizer.param_groups[0]['lr']

        if self._is_better(validation_loss, self.best_loss):
            self.num_bad=0
            self.best_loss=validation_loss
            print(f"‚úì –£–ª—É—á—à–µ–Ω–∏–µ!")
        else:
            self.num_bad+=1
            if self.num_bad > 1:
                self.speed+=1
                print(f"üìâ –£—Å–∫–æ—Ä–µ–Ω–∏–µ! –ù–æ–≤—ã–π speed: {self.speed:.6f}")
            self._reduce_lr()

        return self.optimizer.param_groups[0]['lr']

    def _is_better(self, current, best):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, –ª—É—á—à–µ –ª–∏ —Ç–µ–∫—É—â–∏–π loss"""
        return current < best - best * self.threshold

    def _reduce_lr(self):
        """–£–º–µ–Ω—å—à–µ–Ω–∏–µ LR –¥–ª—è –≤—Å–µ—Ö –≥—Ä—É–ø–ø –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        self.threshold*=self.factor_threshold**self.speed
        factor = self.factor**self.speed
        for param_group in self.optimizer.param_groups:
            old_lr = param_group['lr']
            new_lr = old_lr * factor
            param_group['lr'] = new_lr
        print(f"üìâ –£–º–µ–Ω—å—à–µ–Ω–∏–µ threshold! –ù–æ–≤—ã–π threshold: {self.threshold:.6f}")
        print(f"üìâ –£–º–µ–Ω—å—à–µ–Ω–∏–µ LR! –ù–æ–≤—ã–π LR: {self.optimizer.param_groups[0]['lr']:.6f}")

    def get_last_lr(self):
        return self.optimizer.param_groups[0]['lr']

    def state_dict(self):
        return {
            'max_lr': self.max_lr,
            'ini_lr': self.ini_lr,
            'current_epoch': self.current_epoch,

            'factor': self.factor,
            'threshold': self.threshold,
            'factor_threshold' : self.factor_threshold,
            'num_bad' : self.num_bad,
            'speed' : self.speed,

            'best_loss': self.best_loss,
        }

    def load_state_dict(self, state_dict):
        for key, value in state_dict.items():
            setattr(self, key, value)

# ====================== –ö–û–ú–ü–û–ù–ï–ù–¢–´ –ú–û–î–ï–õ–ò ======================
class MoE(nn.Module):
    def __init__(self, input_dim, num_experts, base_expert_units, k_top):
        super().__init__()
        self.num_experts = num_experts
        self.k_top = k_top
        self.experts = nn.ModuleList()

        # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ä–∞–∑–º–µ—Ä–æ–≤ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –æ—Ç 0.5 –¥–æ 1.5 –æ—Ç –±–∞–∑–æ–≤–æ–≥–æ
        expert_sizes = []
        for i in range(num_experts):
            # –õ–∏–Ω–µ–π–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –æ—Ç 0.5 –¥–æ 1.5
            scale = 0.5 + (i / (num_experts - 1)) if num_experts > 1 else 1.0
            size = int(base_expert_units * scale)
            expert_sizes.append(size)

        print(f"MoE expert sizes: {expert_sizes}")

        for size in expert_sizes:
            self.experts.append(nn.Sequential(
                nn.Linear(input_dim, size),
                nn.BatchNorm1d(size),
                nn.ReLU(inplace=True),
                nn.Dropout(config.dropout),
                nn.Linear(size, input_dim),
                nn.BatchNorm1d(input_dim)
            ))

        self.router = nn.Linear(input_dim, num_experts)

    def forward(self, x):
        logits = self.router(x)
        top_k_weights, top_k_indices = logits.topk(self.k_top, dim=1)
        top_k_weights = torch.softmax(top_k_weights, dim=1)

        # –°–æ–±–∏—Ä–∞–µ–º –≤—ã—Ö–æ–¥—ã –≤—Å–µ—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤
        expert_outputs = []
        for expert in self.experts:
            expert_outputs.append(expert(x))
        expert_outputs = torch.stack(expert_outputs, dim=1)  # [B, num_experts, D]

        # –°–æ–∑–¥–∞–µ–º –º–∞—Å–∫—É –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤
        mask = torch.zeros_like(expert_outputs)
        mask = torch.scatter(
            mask,
            1,
            top_k_indices.unsqueeze(-1).expand(-1, -1, expert_outputs.size(-1)),
            1.0
        )

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Ç–æ–ª—å–∫–æ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤
        expert_outputs = expert_outputs * mask + (expert_outputs * (1 - mask)).detach()

        # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ø-k —ç–∫—Å–ø–µ—Ä—Ç–æ–≤
        selected_outputs = expert_outputs.gather(
            1,
            top_k_indices.unsqueeze(-1).expand(-1, -1, expert_outputs.size(-1))
        )

        # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ
        output = (selected_outputs * top_k_weights.unsqueeze(-1)).sum(dim=1)
        return output + x

class ECABlock(nn.Module):
    def __init__(self, channels, k_size=3):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        # 1D —Å–≤—ë—Ä—Ç–∫–∞ –ø–æ –∫–∞–Ω–∞–ª–∞–º
        self.conv = nn.Conv1d(1, 1, kernel_size=k_size,
                              padding=(k_size - 1) // 2, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        y = self.avg_pool(x)  # [B, C, 1, 1]
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º—É [B, 1, C] –¥–ª—è 1D conv
        y = self.conv(y.squeeze(-1).transpose(-1, -2))
        y = self.sigmoid(y).transpose(-1, -2).unsqueeze(-1)  # [B, C, 1, 1]
        return x * y.expand_as(x)

class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.eca = ECABlock(out_channels)  # üîπ –∑–∞–º–µ–Ω–∏–ª–∏ SE –Ω–∞ ECA
        self.act = nn.ReLU(inplace=True)
        self.dropout = nn.Dropout2d(config.dropout)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        residual = self.shortcut(x)
        x = self.act(self.bn1(self.conv1(x)))
        x = self.dropout(x)
        x = self.bn2(self.conv2(x))
        x = self.eca(x)  # üîπ –∏—Å–ø–æ–ª—å–∑—É–µ–º ECA
        return self.act(x + residual)

class AnimeClassifier(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        # –£–°–ò–õ–ï–ù–ù–´–ô –ë–≠–ö–ë–û–ù: –∫–∞–Ω–∞–ª—ã —É–≤–µ–ª–∏—á–µ–Ω—ã –í 2 –†–ê–ó–ê –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –º–æ—â–Ω–æ—Å—Ç–∏
        self.backbone = nn.Sequential(
            nn.Conv2d(3, 128, 7, stride=2, padding=3, bias=False),     # ‚Üë 64‚Üí128 (2√ó)
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2, padding=1),
            ResidualBlock(128, 128),       # ‚Üë 64‚Üí128 (2√ó)
            ResidualBlock(128, 256, stride=2),  # ‚Üë 128‚Üí256 (2√ó)
            ResidualBlock(256, 512, stride=2),  # ‚Üë 256‚Üí512 (2√ó)
            ResidualBlock(512, 1024, stride=2), # ‚Üë 512‚Üí1024 (2√ó)
            nn.AdaptiveAvgPool2d(1)       # –í—ã—Ö–æ–¥: [B, 1024, 1, 1]
        )
        # MoE —Å 16 —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏ (–≤—Ö–æ–¥ 1024)
        self.moe = MoE(1024, config.num_experts, config.expert_units, config.k_top_expert)
        # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        self.classifier = nn.Sequential(
            nn.Linear(1024, 1024),        # ‚Üë 512‚Üí1024 (2√ó)
            nn.ReLU(inplace=True),
            nn.Dropout(config.dropout),
            nn.Linear(1024, num_classes)  # ‚Üë 512‚Üí1024 (2√ó)
        )

    def forward(self, x):
        x = self.backbone(x).flatten(1)   # [B, 1024]
        x = self.moe(x)
        return self.classifier(x)

# ====================== –û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–• ======================
class ImageDataset(Dataset):
    def __init__(self, root, transform=None, mode='train'):
        # –í—Å–µ–≥–¥–∞ —á–∏—Ç–∞–µ–º –∫–ª–∞—Å—Å—ã –∏–∑ labels.txt
        if os.path.exists(config.labels_path):
            with open(config.labels_path, 'r') as f:
                self.classes = [line.strip() for line in f]
        else:
            # –ï—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ—Ç, –±–µ—Ä–µ–º –∏–∑ –ø–∞–ø–∫–∏ –∏ —Å–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª
            self.classes = sorted(os.listdir(root))
            with open(config.labels_path, 'w') as f:
                f.write('\n'.join(self.classes))

        self.samples = []
        for label, cls in enumerate(self.classes):
            cls_path = os.path.join(root, cls)
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –ø–∞–ø–∫–∞
            if os.path.exists(cls_path):
                self.samples.extend([(f, label) for f in glob(os.path.join(cls_path, '*'))])
            else:
                print(f"‚ö†Ô∏è  –ü–∞–ø–∫–∞ –∫–ª–∞—Å—Å–∞ '{cls}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")

        self.transform = transform or self._get_transforms(mode)

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        img_path, label = self.samples[idx]
        img = np.array(Image.open(img_path).convert('RGB'))

        if self.transform:
            augmented = self.transform(image=img)
            img = augmented['image']

        return img, label

    @staticmethod
    def _get_transforms(mode):
        if mode == 'train':
            return A.Compose([
                # ===================== –í–°–ï–ì–î–ê –ü–†–ò–ú–ï–ù–Ø–ï–¢–°–Ø (p=1.0) =====================
                A.RandomResizedCrop(             # üìè –°–õ–£–ß–ê–ô–ù–û–ï –ö–ê–î–†–ò–†–û–í–ê–ù–ò–ï –° –ò–ó–ú–ï–ù–ï–ù–ò–ï–ú –†–ê–ó–ú–ï–†–ê
                    size=config.input_size,      # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã—Ö–æ–¥–Ω–æ–π —Ä–∞–∑–º–µ—Ä (224x224)
                    scale=(0.8, 1.0),            # –ú–∞—Å—à—Ç–∞–±: 80-100% –æ—Ç –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                                                # (–º–µ–Ω—å—à–µ –∑–Ω–∞—á–µ–Ω–∏–µ ‚Üí –±–æ–ª—å—à–µ –æ–±—Ä–µ–∑–∫–∞)
                    interpolation=1,             # –ú–µ—Ç–æ–¥ –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏ (1=–±–∏–ª–∏–Ω–µ–π–Ω–∞—è)
                    p=1.0                        # ‚úÖ –í–°–ï–ì–î–ê –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è - –æ—Å–Ω–æ–≤–Ω–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è
                ),

                # ===================== –ß–ê–°–¢–û –ü–†–ò–ú–ï–ù–Ø–ï–¢–°–Ø (p=0.5) =====================
                A.Rotate(limit=30, p=0.5),       # üîÑ –ü–û–í–û–†–û–¢
                                                # limit=30: —Å–ª—É—á–∞–π–Ω—ã–π –ø–æ–≤–æ—Ä–æ—Ç ¬±30 –≥—Ä–∞–¥—É—Å–æ–≤
                                                # –£—á–∏—Ç –º–æ–¥–µ–ª—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å –æ–±—ä–µ–∫—Ç—ã –ø–æ–¥ —Ä–∞–∑–Ω—ã–º–∏ —É–≥–ª–∞–º–∏

                A.HorizontalFlip(p=0.5),         # ‚ÜîÔ∏è –ì–û–†–ò–ó–û–ù–¢–ê–õ–¨–ù–û–ï –û–¢–†–ê–ñ–ï–ù–ò–ï
                                                # –õ–µ–≤–æ-–ø—Ä–∞–≤–∞—è —Å–∏–º–º–µ—Ç—Ä–∏—è, —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö

                A.ColorJitter(                   # üé® –¶–í–ï–¢–û–í–´–ï –ò–°–ö–ê–ñ–ï–ù–ò–Ø
                    brightness=0.2,              # –Ø—Ä–∫–æ—Å—Ç—å: ¬±20% (0.8-1.2 –æ—Ç –∏—Å—Ö–æ–¥–Ω–æ–π)
                    contrast=0.2,                # –ö–æ–Ω—Ç—Ä–∞—Å—Ç: ¬±20%
                    saturation=0.2,              # –ù–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç—å: ¬±20%
                    hue=0.2,                     # –û—Ç—Ç–µ–Ω–æ–∫: ¬±0.2 (–≤ –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –µ–¥–∏–Ω–∏—Ü–∞—Ö HSV)
                                                # –ú–µ–Ω—è–µ—Ç —Ü–≤–µ—Ç–∞, –∏–º–∏—Ç–∏—Ä—É–µ—Ç —Ä–∞–∑–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –æ—Å–≤–µ—â–µ–Ω–∏—è
                    p=0.5                        # 50% –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è
                ),

                A.Affine(                           # üìê –ê–§–§–ò–ù–ù–´–ï –ü–†–ï–û–ë–†–ê–ó–û–í–ê–ù–ò–Ø
                    translate_percent=(-0.1, 0.1),  # –°–¥–≤–∏–≥: ¬±10% –æ—Ç —Ä–∞–∑–º–µ—Ä–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    keep_ratio=True,                # –°–æ—Ö—Ä–∞–Ω—è—Ç—å —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Å—Ç–æ—Ä–æ–Ω –ø—Ä–∏ —Å–¥–≤–∏–≥–µ
                    p=0.5                           # 50% –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
                ),

                # ===================== –†–ï–î–ö–û –ü–†–ò–ú–ï–ù–Ø–ï–¢–°–Ø (p=0.2) =====================
                A.GaussianBlur(                  # üå´ –ì–ê–£–°–°–û–í–û –†–ê–ó–ú–´–¢–ò–ï
                    blur_limit=(3, 3),           # –†–∞–∑–º–µ—Ä —è–¥—Ä–∞ —Ä–∞–∑–º—ã—Ç–∏—è: 3√ó3 –ø–∏–∫—Å–µ–ª—è
                                                # (–Ω–µ—á–µ—Ç–Ω–æ–µ —á–∏—Å–ª–æ, –±–æ–ª—å—à–µ ‚Üí —Å–∏–ª—å–Ω–µ–µ —Ä–∞–∑–º—ã—Ç–∏–µ)
                    p=0.2                        # 20% –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å - –∏–º–∏—Ç–∏—Ä—É–µ—Ç —Ä–∞—Å—Ñ–æ–∫—É—Å
                ),

                # ===================== –û–ß–ï–ù–¨ –†–ï–î–ö–û (p=0.1) =====================
                A.RandomGridShuffle(             # üß© –°–õ–£–ß–ê–ô–ù–û–ï –ü–ï–†–ï–ú–ï–®–ò–í–ê–ù–ò–ï –°–ï–¢–ö–ò
                    grid=(2, 2),                 # –†–∞–∑–¥–µ–ª–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞ 2√ó2=4 —á–∞—Å—Ç–∏
                                                # –ò –ø–µ—Ä–µ–º–µ—à–∞—Ç—å –∏—Ö —Å–ª—É—á–∞–π–Ω—ã–º –æ–±—Ä–∞–∑–æ–º
                    p=0.1                        # 10% –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å - —Ä–∞–¥–∏–∫–∞–ª—å–Ω–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è
                ),

                A.Sharpen(                       # üî™ –ü–û–í–´–®–ï–ù–ò–ï –†–ï–ó–ö–û–°–¢–ò
                    alpha=(0.2, 0.5),            # –°–∏–ª–∞ —ç—Ñ—Ñ–µ–∫—Ç–∞: 20-50%
                                                # (0=–Ω–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∞, 1=–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ä–µ–∑–∫–æ—Å—Ç—å)
                    lightness=(0.5, 1.0),        # –Ø—Ä–∫–æ—Å—Ç—å: 50-100%
                    p=0.1                        # 10% –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å - –∏–º–∏—Ç–∏—Ä—É–µ—Ç "–æ—Å—Ç—Ä—ã–π" —Å—Ç–∏–ª—å
                ),

                # ===================== –í–°–ï–ì–î–ê –í –ö–û–ù–¶–ï (–±–µ–∑ p) =====================
                A.ToFloat(max_value=255.0),      # üî¢ –ö–û–ù–í–ï–†–¢–ê–¶–ò–Ø –í FLOAT [0, 1]
                                                # max_value=255: –¥–µ–ª–∏—Ç –Ω–∞ 255 –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏

                ToTensorV2(),                    # ‚ö° –ö–û–ù–í–ï–†–¢–ê–¶–ò–Ø –í –¢–ï–ù–ó–û–† PYTORCH
                                                # –ú–µ–Ω—è–µ—Ç –ø–æ—Ä—è–¥–æ–∫ –æ—Å–µ–π HWC ‚Üí CHW
            ])
        return A.Compose([
            A.Resize(config.input_size[0], config.input_size[1]),  # –î–æ–±–∞–≤–∏–ª–∏ Resize –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            A.ToFloat(max_value=255.0),
            ToTensorV2(),
        ])

# ====================== –û–ë–£–ß–ï–ù–ò–ï ======================
def mixup_data(x, y, alpha=1.0):
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1
    batch_size = x.size(0)
    index = torch.randperm(batch_size).to(x.device)

    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam

def cutmix_data(x, y, alpha=1.0):
    """
    –°–º–µ—à–∏–≤–∞–µ—Ç –ª–µ–≤—É—é –ø–æ–ª–æ–≤–∏–Ω—É –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø—Ä–∞–≤–æ–π –ø–æ–ª–æ–≤–∏–Ω–æ–π –¥—Ä—É–≥–æ–≥–æ
    """
    batch_size, _, h, w = x.size()
    index = torch.randperm(batch_size).to(x.device)

    # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ—á–∫—É —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è (–º–æ–∂–µ—Ç –±—ã—Ç—å —Å–ª—É—á–∞–π–Ω–æ–π)
    split_point = np.random.randint(int(0.4 * w), int(0.6 * w))

    # –°–æ–∑–¥–∞—ë–º —Å–º–µ—à–∞–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    mixed_x = x.clone()

    # –õ–ï–í–ê–Ø —á–∞—Å—Ç—å –æ—Ç –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    mixed_x[:, :, :, :split_point] = x[:, :, :, :split_point]

    # –ü–†–ê–í–ê–Ø —á–∞—Å—Ç—å –æ—Ç –¥—Ä—É–≥–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    mixed_x[:, :, :, split_point:] = x[index, :, :, split_point:]

    y_a, y_b = y, y[index]
    lam = split_point / w

    return mixed_x, y_a, y_b, lam

def get_class_weights_from_dirs(root_dir, class_names):
    class_counts = []
    for class_name in class_names:
        path = os.path.join(root_dir, class_name)
        count = len(glob(os.path.join(path, "*")))
        class_counts.append(count)

    total = sum(class_counts)
    weights = [total / (count + 1e-6) for count in class_counts]  # –∑–∞—â–∏—Ç–∞ –æ—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0
    weights = torch.tensor(weights)
    weights = weights / weights.max()  # –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    return weights

def focal_loss_with_smoothing(outputs, targets, gamma, smoothing, class_weights=None):
    num_classes = outputs.size(1)
    confidence = 1.0 - smoothing

    log_probs = torch.nn.functional.log_softmax(outputs, dim=-1)
    probs = torch.exp(log_probs)

    true_dist = torch.full_like(log_probs, smoothing / (num_classes - 1))
    true_dist.scatter_(1, targets.unsqueeze(1), confidence)

    pt = torch.sum(true_dist * probs, dim=-1)
    focal_factor = (1 - pt).pow(gamma)
    loss = -torch.sum(true_dist * log_probs, dim=-1)

    if class_weights is not None:
        weights = class_weights[targets]
        loss = loss * weights

    return torch.mean(focal_factor * loss)

def forward_with_mixup_cutmix(model, inputs, labels, config, class_weights, device):
    inputs, labels = inputs.to(device), labels.to(device)

    use_mix = np.random.rand() < 0.50
    use_cutmix = np.random.rand() < 0.5

    if use_mix:
        if use_cutmix:
            inputs, targets_a, targets_b, lam = cutmix_data(inputs, labels, alpha=1.0)
        else:
            inputs, targets_a, targets_b, lam = mixup_data(inputs, labels, alpha=1.0)

        with torch.amp.autocast('cuda', enabled=config.mixed_precision):
            outputs = model(inputs)
            loss = lam * focal_loss_with_smoothing(outputs, targets_a, config.focal_gamma, config.smoothing, class_weights)\
                 + (1 - lam) * focal_loss_with_smoothing(outputs, targets_b, config.focal_gamma, config.smoothing, class_weights)
    else:
        with torch.amp.autocast('cuda', enabled=config.mixed_precision):
            outputs = model(inputs)
            loss = focal_loss_with_smoothing(outputs, labels, config.focal_gamma, config.smoothing, class_weights)

    return outputs, loss

def compile_model(model):
    torch.compile(model,
        mode="max-autotune",
        dynamic=False,
        fullgraph=True)
    torch.cuda.empty_cache()

def save_checkpoint(model, epoch, optimizer, scheduler, path):
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ –±–µ–∑ —Å–∂–∞—Ç–∏—è
    """
    # 1. –°–æ–∑–¥–∞—ë–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
    }

    # 2. –°–æ—Ö—Ä–∞–Ω—è–µ–º –±–µ–∑ —Å–∂–∞—Ç–∏—è
    torch.save(checkpoint, path, pickle_protocol=5)  # –ü—Ä–æ—Ç–æ–∫–æ–ª 5 –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏

    # 3. –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
    size_mb = os.path.getsize(path) / 1024 / 1024
    print(f"[System]  –ß–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {size_mb:.1f} MB")

    return size_mb

def load_checkpoint(model, optimizer, scheduler, path, device):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
    """
    try:
        # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞–ø—Ä—è–º—É—é (–±–µ–∑ gzip)
        checkpoint = torch.load(path, map_location='cpu', weights_only=False)

        # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ –Ω–∞–ø—Ä—è–º—É—é (—Ç–∏–ø—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(device)

        # 3. –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º optimizer –∏ scheduler
        if optimizer and 'optimizer_state_dict' in checkpoint and checkpoint['optimizer_state_dict']:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

        if scheduler and 'scheduler_state_dict' in checkpoint and checkpoint['scheduler_state_dict']:
            if hasattr(scheduler, 'load_state_dict'):
                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            else:
                print("‚ö†Ô∏è  Scheduler –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç load_state_dict")

        print(f"‚úì –ß–µ–∫–ø–æ–∏–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω")
        print(f"  –≠–ø–æ—Ö–∞: {checkpoint['epoch']}")

        return {
            'model': model,
            'epoch': checkpoint['epoch'],
            'optimizer': optimizer,
            'scheduler': scheduler,
        }

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
        import traceback
        traceback.print_exc()
        return None

def make_sound():
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–π "–±–∏–ø" (440 –ì—Ü, 0.2 —Å–µ–∫—É–Ω–¥—ã)
    sample_rate = 44100
    duration = 0.2  # —Å–µ–∫—É–Ω–¥—ã
    t = np.linspace(0, duration, int(sample_rate * duration))
    audio_signal = 0.3 * np.sin(2 * np.pi * 440 * t)  # 440 –ì—Ü - –Ω–æ—Ç–∞ "–õ—è"

    # –í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
    display(Audio(audio_signal, rate=sample_rate, autoplay=True))

def run_training():
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—á–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
    torch.set_float32_matmul_precision('medium')

    # –í–∫–ª—é—á–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ cuDNN
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    os.makedirs(os.path.dirname(config.checkpoint_path), exist_ok=True)

    full_classes = sorted(os.listdir(config.source_dir))

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º/–ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º —Å–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Å–æ–≤
    with open(config.labels_path, 'w') as f:
        f.write('\n'.join(full_classes))

    print(f"üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(full_classes)}")
    print(f"üöÄ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è:")
    print(f"  ‚Ä¢ Ini LR: {config.ini_lr:.4f}")
    print(f"  ‚Ä¢ Max LR: {config.max_lr:.4f}")
    print(f"  ‚Ä¢ –í—Å–µ–≥–æ —ç–ø–æ—Ö: {config.epochs}")

    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø–æ—Ä—è–¥–∫–æ–º –∫–ª–∞—Å—Å–æ–≤
    full_dataset = ImageDataset(config.source_dir)
    full_dataset.classes = full_classes

    train_size = int((1 - config.val_split) * len(full_dataset))
    train_ds, val_ds = torch.utils.data.random_split(full_dataset, [train_size, len(full_dataset) - train_size])

    train_loader = DataLoader(
        train_ds,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=2,
        persistent_workers=True,
        prefetch_factor=4,
        pin_memory=True)
    val_loader = DataLoader(
        val_ds,
        batch_size=config.batch_size,
        num_workers=2,
        persistent_workers=True,
        prefetch_factor=4,
        pin_memory=True)

    model = AnimeClassifier(len(full_classes)).to(device)

    class_weights = get_class_weights_from_dirs(config.source_dir, full_classes).to(device)

    optimizer = optim.AdamW(model.parameters(), lr=config.ini_lr)

    scheduler = WarmupReduceLROnPlateau(
        optimizer=optimizer,
        ini_lr=config.ini_lr,
        max_lr=config.max_lr,
        factor=config.plateau_factor,
        factor_threshold=config.plateau_factor_threshold,
        threshold=config.plateau_threshold,
    )

    scaler = torch.amp.GradScaler('cuda', enabled=config.mixed_precision and torch.cuda.is_available())
    start_epoch = 1
    early_stop_counter = 0

    if config.resume_training:
        loaded = load_checkpoint(model, optimizer, scheduler, config.checkpoint_path, device)

        if loaded is not None:
            model = loaded['model']
            optimizer = loaded['optimizer']
            scheduler = loaded['scheduler']
            start_epoch = loaded['epoch'] + 1

            # –ö–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏
            compile_model(model)

            print(f"üîÑ –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å —ç–ø–æ—Ö–∏ {start_epoch}")
            print(f"  –¢–µ–∫—É—â–∏–π LR: {optimizer.param_groups[0]['lr']:.6f}")
        else:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç, –Ω–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è")
            compile_model(model)
            save_checkpoint(
                model=model,
                epoch=-1,
                optimizer=optimizer,
                scheduler=scheduler,
                path=config.checkpoint_path
            )
    else:
        compile_model(model)
        save_checkpoint(
            model=model,
            epoch=-1,
            optimizer=optimizer,
            scheduler=scheduler,
            path=config.checkpoint_path
        )

    start_time = time.time()
    train_loader_len = len(train_loader)
    val_loader_len = len(val_loader)

    for epoch in range(start_epoch, config.epochs+1):
        make_sound()

        model.train()
        train_loss = 0.0
        train_correct, train_total = 0, 0
        optimizer.zero_grad(set_to_none=True)
        epoch_start_time = time.time()

        current_lr = scheduler.get_last_lr()
        print(f"[LR] Current: {current_lr:.8f}")

        batch_start_time = time.time()
        for batch_idx, (inputs, labels) in enumerate(train_loader):
            inputs = inputs.to(device, non_blocking=True)  # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –ø–µ—Ä–µ–¥–∞—á–∞
            labels = labels.to(device, non_blocking=True)

            outputs, loss = forward_with_mixup_cutmix(model, inputs, labels, config, class_weights, device)

            scaler.scale(loss).backward()
            train_loss += loss.item()

            # –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –∫–ª–∏–ø–ø–∏–Ω–≥
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config.gradient_clip)

            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad(set_to_none=True)

            _, predicted = torch.max(outputs, 1)
            current_batch_size = labels.size(0)
            train_total += current_batch_size
            train_correct += (predicted == labels).sum().item()

            batch_duration = (time.time() - batch_start_time) / (batch_idx + 1)
            remaining_batches = train_loader_len - (batch_idx + 1)
            estimated_remaining_time = remaining_batches * batch_duration
            remaining_time_str = time.strftime('%H:%M:%S', time.gmtime(estimated_remaining_time))

            print(
                f"\r[Train] Epoch {epoch}/{config.epochs} | Batch {batch_idx+1}/{train_loader_len} | "
                f"Loss: {(loss.item()):.4f} | Remaining time: {remaining_time_str}",
                end='', flush=True)

        train_accuracy = 100 * train_correct / train_total
        print()

        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        model.eval()
        val_loss, val_correct, val_total = 0.0, 0, 0
        all_preds, all_labels = [], []

        batch_start_time = time.time()
        with torch.inference_mode(), torch.amp.autocast('cuda', enabled=config.mixed_precision):
            for batch_idx, (inputs, labels) in enumerate(val_loader):
                inputs = inputs.to(device, non_blocking=True)  # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –ø–µ—Ä–µ–¥–∞—á–∞
                labels = labels.to(device, non_blocking=True)

                outputs = model(inputs)

                loss = focal_loss_with_smoothing(outputs, labels, config.focal_gamma, config.smoothing)
                val_loss += loss.item()

                _, predicted = torch.max(outputs, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()

                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())

                batch_duration = (time.time() - batch_start_time) / (batch_idx + 1)
                remaining_batches = val_loader_len - (batch_idx + 1)
                estimated_remaining_time = remaining_batches * batch_duration
                remaining_time_str = time.strftime('%H:%M:%S', time.gmtime(estimated_remaining_time))

                print(
                    f"\r[Val]   Epoch {epoch}/{config.epochs} | Batch {batch_idx+1}/{val_loader_len} | "
                    f"Loss: {loss.item():.4f} | Remaining: {remaining_time_str}",
                    end='', flush=True)

        print()

        # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
        val_accuracy = 100 * val_correct / val_total
        val_precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)
        val_recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)
        val_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)

        epoch_end_time = time.time()
        epoch_duration = epoch_end_time - epoch_start_time
        total_elapsed_time = epoch_end_time - start_time
        epoch_duration_str = time.strftime("%H:%M:%S", time.gmtime(epoch_duration))
        total_elapsed_str = time.strftime("%H:%M:%S", time.gmtime(total_elapsed_time))

        # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
        if val_loss < scheduler.best_loss:
            early_stop_counter = 0
            save_checkpoint(
                model=model,
                epoch=epoch,
                optimizer=optimizer,
                scheduler=scheduler,
                path=config.checkpoint_path
            )
        else:
            early_stop_counter += 1
            if early_stop_counter >= config.early_stopping_patience:
                print(f"[System]  Early Stop (no improvement for {early_stop_counter} epochs)")
                break

        next_lr = scheduler.step(epoch+1, val_loss)

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        print(f"[Summary] Train Loss: {train_loss/train_loader_len:.4f} | Acc: {train_accuracy:.2f}%")
        print(f"[Summary] Val   Loss: {val_loss/val_loader_len:.4f} | Acc: {val_accuracy:.2f}%")
        print(f"[Summary] Val Precision: {val_precision:.4f} | Recall: {val_recall:.4f} | F1: {val_f1:.4f}")
        print(f"[Time]    Epoch: {epoch_duration_str} | Total: {total_elapsed_str}")
        print(f"[LR]      Current: {current_lr:.8f}")
        print(f"[LR]      Next:    {next_lr:.8f}")

        print()

# ====================== –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ======================
def convert_to_onnx():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    with open(config.labels_path) as f:
        classes = [line.strip() for line in f]

    model = AnimeClassifier(len(classes)).to(device)
    optimizer = optim.AdamW(model.parameters(), lr=config.ini_lr)
    scheduler = WarmupReduceLROnPlateau(
        optimizer=optimizer,
        ini_lr=config.ini_lr,
        max_lr=config.max_lr,
        factor=config.plateau_factor,
        factor_threshold=config.plateau_factor_threshold,
        threshold=config.plateau_threshold,
    )

    loaded = load_checkpoint(model, optimizer, scheduler, config.checkpoint_path, device)

    if loaded is None:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ ONNX!")
        return

    model = loaded['model']
    model.eval()

    print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –¥–ª—è ONNX —ç–∫—Å–ø–æ—Ä—Ç–∞")
    print(f"   –ö–ª–∞—Å—Å–æ–≤: {len(classes)}")

    dummy_input = torch.randn(1, 3, *config.input_size).to(device)

    try:
        torch.onnx.export(
            model,
            dummy_input,
            config.onnx_path,
            input_names=['input'],
            output_names=['output'],
            dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}},
            do_constant_folding=True,
            opset_version=14,
            training=torch.onnx.TrainingMode.EVAL,
            verbose=False
        )
        print(f"‚úÖ ONNX –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {config.onnx_path}")

        if os.path.exists(config.onnx_path):
            size_mb = os.path.getsize(config.onnx_path) / 1024 / 1024
            print(f"   –†–∞–∑–º–µ—Ä ONNX —Ñ–∞–π–ª–∞: {size_mb:.2f} MB")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ –≤ ONNX: {e}")
        import traceback
        traceback.print_exc()

def test_onnx():
    if not os.path.exists(config.onnx_path):
        print("‚ùå ONNX –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
        print(f"   –ü—É—Ç—å: {config.onnx_path}")
        return

    try:
        options = ort.SessionOptions()
        options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL

        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if torch.cuda.is_available() else ['CPUExecutionProvider']

        session = ort.InferenceSession(config.onnx_path, options, providers=providers)
        print("‚úÖ ONNX Runtime —Å–µ—Å—Å–∏—è —Å–æ–∑–¥–∞–Ω–∞")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ ONNX –º–æ–¥–µ–ª–∏: {e}")
        return

    test_image_path = os.path.join("/content/NeuralNetwork-ImageClassifier/", "test.jpg")
    if not os.path.exists(test_image_path):
        print(f"‚ùå –¢–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {test_image_path}")
        print("   –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª test.jpg –≤ –ø–∞–ø–∫–µ –ø—Ä–æ–µ–∫—Ç–∞")
        return

    try:
        img = Image.open(test_image_path).convert('RGB')
        img_np = np.array(img)

        transform = ImageDataset._get_transforms('val')
        augmented = transform(image=img_np)
        img_tensor = augmented['image'].unsqueeze(0)

        print(f"‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {img.size[0]}x{img.size[1]}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
        return

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    with open(config.labels_path) as f:
        classes = [line.strip() for line in f]

    model = AnimeClassifier(len(classes)).to(device)
    optimizer = optim.AdamW(model.parameters(), lr=config.ini_lr)
    scheduler = WarmupReduceLROnPlateau(
        optimizer=optimizer,
        ini_lr=config.ini_lr,
        max_lr=config.max_lr,
        factor=config.plateau_factor,
        factor_threshold=config.plateau_factor_threshold,
        threshold=config.plateau_threshold,
    )

    loaded = load_checkpoint(model, optimizer, scheduler, config.checkpoint_path, device)

    if loaded is None:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å PyTorch –º–æ–¥–µ–ª—å –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è!")
        return

    model = loaded['model']
    model.eval()

    with torch.no_grad():
        pytorch_output = model(img_tensor.to(device))
        pytorch_probs = torch.softmax(pytorch_output, dim=1).cpu()

    try:
        onnx_input = img_tensor.numpy().astype(np.float32)
        onnx_outputs = session.run(None, {'input': onnx_input})
        onnx_probs = torch.softmax(torch.tensor(onnx_outputs[0]), dim=1)

        print("‚úÖ ONNX inference –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ ONNX inference: {e}")
        return

    print("\n" + "="*50)
    print("[PyTorch] –¢–æ–ø-5 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
    pytorch_top_probs, pytorch_top_indices = torch.topk(pytorch_probs, 5)
    for i, (prob, idx) in enumerate(zip(pytorch_top_probs[0], pytorch_top_indices[0])):
        print(f"{i+1}. {classes[idx]}: {prob.item()*100:.2f}%")

    print("\n[ONNX] –¢–æ–ø-5 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
    onnx_top_probs, onnx_top_indices = torch.topk(onnx_probs, 5)
    for i, (prob, idx) in enumerate(zip(onnx_top_probs[0], onnx_top_indices[0])):
        print(f"{i+1}. {classes[idx]}: {prob.item()*100:.2f}%")

    diff = torch.max(torch.abs(pytorch_probs - onnx_probs)).item()
    print(f"\n[–°—Ä–∞–≤–Ω–µ–Ω–∏–µ] –†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ –º–µ–∂–¥—É –≤—ã—Ö–æ–¥–∞–º–∏: {diff:.6f}")

    if diff < 0.001:
        print("‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞! –†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ < 0.001")
    elif diff < 0.01:
        print("‚ö†Ô∏è  –ù–µ–±–æ–ª—å—à–æ–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ (0.001-0.01), –≤–æ–∑–º–æ–∂–Ω–æ –∏–∑-–∑–∞ —á–∏—Å–ª–µ–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏")
    else:
        print("‚ùå –ë–æ–ª—å—à–æ–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ (> 0.01)! –í–æ–∑–º–æ–∂–Ω–∞—è –æ—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏")

    print("\n" + "="*50)
    print(f"PyTorch device: {device}")
    print(f"PyTorch dtype: {pytorch_probs.dtype}")
    print(f"ONNX dtype: {onnx_probs.dtype}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(classes)}")

# ====================== –ò–ù–¢–ï–†–§–ï–ô–° ======================
def main_menu():
    while True:
        print("\n" + "="*50)
        print("üöÄ Anime Classifier")
        print("="*50)
        print("1. –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å (—Å –Ω—É–ª—è)")
        print("2. –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ")
        print("3. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ ONNX")
        print("4. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å ONNX")
        choice = input("–í—ã–±–æ—Ä: ").strip()

        if choice == '1':
            config.resume_training = False
            run_training()
        elif choice == '2':
            if not os.path.exists(config.checkpoint_path):
                print("‚ùå –ß–µ–∫–ø–æ–∏–Ω—Ç –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω!")
                continue
            config.resume_training = True
            run_training()
        elif choice == '3':
            convert_to_onnx()
        elif choice == '4':
            test_onnx()
        elif choice == '0':
            break
        else:
            print("–ù–µ–≤–µ—Ä–Ω—ã–π –≤–≤–æ–¥!")

if __name__ == "__main__":
    main_menu()