import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
import os
from glob import glob
from PIL import Image
import onnxruntime as ort
from sklearn.metrics import precision_score, recall_score, f1_score
import time
from torchinfo import summary
import albumentations as A
from albumentations.pytorch import ToTensorV2
import torch.backends.cudnn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import gzip

# ====================== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ======================
class Config:
    # –≤—ã–±–æ—Ä —Å–∏—Å—Ç–µ–º—ã
    system = "colab"

    match system:
        case "my":
            notebook = False
            dir = "/media/alex/Games/WPS/NeuralNetwork-ImageClassifier/"
        case "colab":
            notebook = True
            dir = "/content/NeuralNetwork-ImageClassifier/"

    # –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º –∏ –º–æ–¥–µ–ª—è–º
    source_dir = dir + "DataSet/ARTS/Original"         # –ü–∞–ø–∫–∞ —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
    checkpoint_path = dir + "Model/best_model.pth"     # –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è/–∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
    labels_path = dir + "Model/labels.txt"             # –§–∞–π–ª —Å –º–µ—Ç–∫–∞–º–∏ –∫–ª–∞—Å—Å–æ–≤
    onnx_path = dir + "Model/model.onnx"               # –ü—É—Ç—å –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –≤ ONNX —Ñ–æ—Ä–º–∞—Ç–µ

    # –§–ª–∞–≥–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏–µ–º
    resume_training = False         # –ü—Ä–æ–¥–æ–ª–∂–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞, –µ—Å–ª–∏ True

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    input_size = (224, 224)         # –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (—à–∏—Ä–∏–Ω–∞, –≤—ã—Å–æ—Ç–∞)

    # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ –∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    num_experts = 32                # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –≤ MoE (Mixture of Experts)
    expert_units = 1024             # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ –∫–∞–∂–¥–æ–º —ç–∫—Å–ø–µ—Ä—Ç–µ
    k_top_expert = 8                # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –Ω–∞ –æ–¥–∏–Ω –ø—Ä–∏–º–µ—Ä
    se_reduction = 16               # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Ä–µ–¥—É–∫—Ü–∏–∏ –¥–ª—è SE (Squeeze-and-Excitation) –±–ª–æ–∫–∞
    dropout = 0.5                   # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—Ç–∫–ª—é—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–æ–≤ (dropout)

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    lr = 0.002                      # –ù–∞—á–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (learning rate)
    batch_size = 512                # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (—á–∏—Å–ª–æ –ø—Ä–∏–º–µ—Ä–æ–≤, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã—Ö –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥)
    epochs = 100                    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è (–ø–æ–ª–Ω—ã—Ö –ø—Ä–æ—Ö–æ–¥–æ–≤ –ø–æ –≤—Å–µ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É)
    focal_gamma = 5                 # –ü–∞—Ä–∞–º–µ—Ç—Ä –≥–∞–º–º–∞ –¥–ª—è Focal Loss, —Ä–µ–≥—É–ª–∏—Ä—É–µ—Ç —Å—Ç–µ–ø–µ–Ω—å —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–∏ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö
    smoothing = 0.1                 # –ü–∞—Ä–∞–º–µ—Ç—Ä label smoothing, –∑–∞–¥–∞—ë—Ç —É—Ä–æ–≤–µ–Ω—å —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è –º–µ—Ç–æ–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±–æ–±—â–µ–Ω–∏—è

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ –∫–æ–Ω—Ç—Ä–æ–ª—è –æ–±—É—á–µ–Ω–∏—è
    mixed_precision = True          # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–º–µ—à–∞–Ω–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å (fp16) –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
    early_stopping_patience = 5     # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è –¥–ª—è —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
    val_split = 0.2                 # –î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö, –≤—ã–¥–µ–ª—è–µ–º–∞—è –ø–æ–¥ –≤–∞–ª–∏–¥–∞—Ü–∏—é
    factor_lr = 0.5                 # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —É–º–µ–Ω—å—à–µ–Ω–∏—è learning rate –ø—Ä–∏ plateau

config = Config()

# ====================== –ö–û–ú–ü–û–ù–ï–ù–¢–´ –ú–û–î–ï–õ–ò ======================
class MoE(nn.Module):
    def __init__(self, input_dim, num_experts, base_expert_units, k_top):
        super().__init__()
        self.num_experts = num_experts
        self.k_top = k_top
        self.experts = nn.ModuleList()

        # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ä–∞–∑–º–µ—Ä–æ–≤ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –æ—Ç 0.5 –¥–æ 1.5 –æ—Ç –±–∞–∑–æ–≤–æ–≥–æ
        expert_sizes = []
        for i in range(num_experts):
            # –õ–∏–Ω–µ–π–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –æ—Ç 0.5 –¥–æ 1.5
            scale = 0.5 + (i / (num_experts - 1)) if num_experts > 1 else 1.0
            size = int(base_expert_units * scale)
            expert_sizes.append(size)

        print(f"MoE expert sizes: {expert_sizes}")

        for size in expert_sizes:
            self.experts.append(nn.Sequential(
                nn.Linear(input_dim, size),
                nn.BatchNorm1d(size),
                nn.ReLU(inplace=True),
                nn.Dropout(config.dropout),
                nn.Linear(size, input_dim),
                nn.BatchNorm1d(input_dim)
            ))

        self.router = nn.Linear(input_dim, num_experts)

    def forward(self, x):
        logits = self.router(x)
        top_k_weights, top_k_indices = logits.topk(self.k_top, dim=1)
        top_k_weights = torch.softmax(top_k_weights, dim=1)

        # –°–æ–±–∏—Ä–∞–µ–º –≤—ã—Ö–æ–¥—ã –≤—Å–µ—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤
        expert_outputs = []
        for expert in self.experts:
            expert_outputs.append(expert(x))
        expert_outputs = torch.stack(expert_outputs, dim=1)  # [B, num_experts, D]

        # –°–æ–∑–¥–∞–µ–º –º–∞—Å–∫—É –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤
        mask = torch.zeros_like(expert_outputs)
        mask = torch.scatter(
            mask,
            1,
            top_k_indices.unsqueeze(-1).expand(-1, -1, expert_outputs.size(-1)),
            1.0
        )

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Ç–æ–ª—å–∫–æ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤
        expert_outputs = expert_outputs * mask + (expert_outputs * (1 - mask)).detach()

        # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ø-k —ç–∫—Å–ø–µ—Ä—Ç–æ–≤
        selected_outputs = expert_outputs.gather(
            1,
            top_k_indices.unsqueeze(-1).expand(-1, -1, expert_outputs.size(-1))
        )

        # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ
        output = (selected_outputs * top_k_weights.unsqueeze(-1)).sum(dim=1)
        return output + x

class ECABlock(nn.Module):
    def __init__(self, channels, k_size=3):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        # 1D —Å–≤—ë—Ä—Ç–∫–∞ –ø–æ –∫–∞–Ω–∞–ª–∞–º
        self.conv = nn.Conv1d(1, 1, kernel_size=k_size,
                              padding=(k_size - 1) // 2, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        y = self.avg_pool(x)  # [B, C, 1, 1]
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º—É [B, 1, C] –¥–ª—è 1D conv
        y = self.conv(y.squeeze(-1).transpose(-1, -2))
        y = self.sigmoid(y).transpose(-1, -2).unsqueeze(-1)  # [B, C, 1, 1]
        return x * y.expand_as(x)

class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.eca = ECABlock(out_channels)  # üîπ –∑–∞–º–µ–Ω–∏–ª–∏ SE –Ω–∞ ECA
        self.act = nn.ReLU(inplace=True)
        self.dropout = nn.Dropout2d(config.dropout)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        residual = self.shortcut(x)
        x = self.act(self.bn1(self.conv1(x)))
        x = self.dropout(x)
        x = self.bn2(self.conv2(x))
        x = self.eca(x)  # üîπ –∏—Å–ø–æ–ª—å–∑—É–µ–º ECA
        return self.act(x + residual)

class AnimeClassifier(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.backbone = nn.Sequential(
            nn.Conv2d(3, 64, 7, stride=2, padding=3, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2, padding=1),
            ResidualBlock(64, 64),
            ResidualBlock(64, 128, stride=2),
            ResidualBlock(128, 256, stride=2),
            ResidualBlock(256, 512, stride=2),
            nn.AdaptiveAvgPool2d(1)
        )
        self.moe = MoE(512, config.num_experts, config.expert_units, config.k_top_expert)
        self.classifier = nn.Sequential(
            nn.Linear(512, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(config.dropout),
            nn.Linear(512, num_classes)
        )

    def forward(self, x):
        x = self.backbone(x).flatten(1)
        x = self.moe(x)
        return self.classifier(x)

# ====================== –û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–• ======================
class ImageDataset(Dataset):
    def __init__(self, root, transform=None, mode='train'):
        # –í—Å–µ–≥–¥–∞ —á–∏—Ç–∞–µ–º –∫–ª–∞—Å—Å—ã –∏–∑ labels.txt
        if os.path.exists(config.labels_path):
            with open(config.labels_path, 'r') as f:
                self.classes = [line.strip() for line in f]
        else:
            # –ï—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ—Ç, –±–µ—Ä–µ–º –∏–∑ –ø–∞–ø–∫–∏ –∏ —Å–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª
            self.classes = sorted(os.listdir(root))
            with open(config.labels_path, 'w') as f:
                f.write('\n'.join(self.classes))

        self.samples = []
        for label, cls in enumerate(self.classes):
            cls_path = os.path.join(root, cls)
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –ø–∞–ø–∫–∞
            if os.path.exists(cls_path):
                self.samples.extend([(f, label) for f in glob(os.path.join(cls_path, '*'))])
            else:
                print(f"‚ö†Ô∏è  –ü–∞–ø–∫–∞ –∫–ª–∞—Å—Å–∞ '{cls}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")

        self.transform = transform or self._get_transforms(mode)

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        img_path, label = self.samples[idx]
        img = np.array(Image.open(img_path).convert('RGB'))  # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ numpy array

        if self.transform:
            augmented = self.transform(image=img)
            img = augmented['image']

        return img, label

    @staticmethod
    def _get_transforms(mode):
        if mode == 'train':
            return A.Compose([
                A.Rotate(limit=30, p=0.5),
                A.RandomResizedCrop(
                    size=config.input_size,
                    scale=(0.8, 1.0),
                    ratio=(0.75, 1.33),          # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (0.75, 1.33))
                    interpolation=1,             # BILINEAR
                    p=1.0
                ),
                A.HorizontalFlip(p=0.5),
                A.ColorJitter(
                    brightness=0.2,
                    contrast=0.2,
                    saturation=0.2,
                    hue=0.0,                     # –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä
                    p=0.5
                ),
                A.GaussianBlur(blur_limit=(3, 3), p=0.2),
                A.Affine(
                    translate_percent=(-0.1, 0.1),
                    keep_ratio=True,
                    p=0.5
                ),
                A.ToFloat(max_value=255.0),
                ToTensorV2(),
            ])
        return A.Compose([
            A.ToFloat(max_value=255.0),
            ToTensorV2(),
        ])

# ====================== –û–ë–£–ß–ï–ù–ò–ï ======================
def mixup_data(x, y, alpha=1.0):
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1
    batch_size = x.size(0)
    index = torch.randperm(batch_size).to(x.device)

    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam

def cutmix_data(x, y, alpha=1.0):
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1
    batch_size, _, h, w = x.size()
    index = torch.randperm(batch_size).to(x.device)

    cx = np.random.randint(w)
    cy = np.random.randint(h)
    cut_w = int(w * np.sqrt(1 - lam))
    cut_h = int(h * np.sqrt(1 - lam))

    x1 = np.clip(cx - cut_w // 2, 0, w)
    x2 = np.clip(cx + cut_w // 2, 0, w)
    y1 = np.clip(cy - cut_h // 2, 0, h)
    y2 = np.clip(cy + cut_h // 2, 0, h)

    x[:, :, y1:y2, x1:x2] = x[index, :, y1:y2, x1:x2]
    y_a, y_b = y, y[index]
    lam = 1 - ((x2 - x1) * (y2 - y1) / (w * h))
    return x, y_a, y_b, lam

def get_class_weights_from_dirs(root_dir, class_names):
    class_counts = []
    for class_name in class_names:
        path = os.path.join(root_dir, class_name)
        count = len(glob(os.path.join(path, "*")))
        class_counts.append(count)

    total = sum(class_counts)
    weights = [total / (count + 1e-6) for count in class_counts]  # –∑–∞—â–∏—Ç–∞ –æ—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0
    weights = torch.tensor(weights)
    weights = weights / weights.mean()  # –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    return weights

def focal_loss_with_smoothing(outputs, targets, gamma=5.0, smoothing=0.1, class_weights=None):
    num_classes = outputs.size(1)
    confidence = 1.0 - smoothing

    log_probs = torch.nn.functional.log_softmax(outputs, dim=-1)
    probs = torch.exp(log_probs)

    true_dist = torch.full_like(log_probs, smoothing / (num_classes - 1))
    true_dist.scatter_(1, targets.unsqueeze(1), confidence)

    pt = torch.sum(true_dist * probs, dim=-1)
    focal_factor = (1 - pt).pow(gamma)
    loss = -torch.sum(true_dist * log_probs, dim=-1)

    if class_weights is not None:
        weights = class_weights[targets]
        loss = loss * weights

    return torch.mean(focal_factor * loss)

def imshow(img_tensor, title=None):
    # img_tensor: Tensor —Å —Ñ–æ—Ä–º–∞—Ç–æ–º (C, H, W)
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–Ω–∑–æ—Ä –≤ numpy –¥–ª—è matplotlib –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ [0,1]
    img = img_tensor.cpu().numpy()
    img = np.transpose(img, (1, 2, 0))  # C,H,W -> H,W,C
    img = np.clip(img, 0, 1)  # –ß—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º —Å —Ü–≤–µ—Ç–∞–º–∏

    plt.imshow(img)
    if title:
        plt.title(title)
    plt.axis('off')
    plt.savefig('output_image.png')
    plt.close()

def forward_with_mixup_cutmix(model, inputs, labels, config, class_weights, device):
    inputs, labels = inputs.to(device), labels.to(device)

    use_mix = np.random.rand() < 0.50
    use_cutmix = np.random.rand() < 0.5

    if use_mix:
        if use_cutmix:
            inputs, targets_a, targets_b, lam = cutmix_data(inputs, labels, alpha=1.0)
        else:
            inputs, targets_a, targets_b, lam = mixup_data(inputs, labels, alpha=1.0)

        #imshow(inputs[1], title="Original image")

        with torch.amp.autocast('cuda', enabled=config.mixed_precision):
            outputs = model(inputs)
            loss = lam * focal_loss_with_smoothing(outputs, targets_a, config.focal_gamma, config.smoothing, class_weights)\
                 + (1 - lam) * focal_loss_with_smoothing(outputs, targets_b, config.focal_gamma, config.smoothing, class_weights)
    else:
        with torch.amp.autocast('cuda', enabled=config.mixed_precision):
            outputs = model(inputs)
            loss = focal_loss_with_smoothing(outputs, labels, config.focal_gamma, config.smoothing, class_weights)

    return outputs, loss

def compile_model(model):
    torch.compile(model,
        mode="max-autotune",
        dynamic=False,
        fullgraph=False)
    torch.cuda.empty_cache()

def save_compressed_checkpoint(model, epoch, best_loss, lr, path):
    """
    –£–º–Ω–æ–µ —Å–∂–∞—Ç–∏–µ —Å —É—á–µ—Ç–æ–º mixed precision
    """
    # 1. –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫–∏–µ –≤–µ—Å–∞ –º–æ–∂–Ω–æ —Å–∂–∏–º–∞—Ç—å
    checkpoint = {
        'epoch': epoch,
        'best_loss': best_loss,
        'learning_rate': lr,
    }

    # 2. –°–∂–∏–º–∞–µ–º –í–°–ï –≤–µ—Å–∞ –≤ float16 (–¥–∞–∂–µ –µ—Å–ª–∏ –æ–Ω–∏ float32)
    compressed_weights = {}
    for name, param in model.state_dict().items():
        if param.is_floating_point():
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –≤ float16 –¥–ª—è —Å–∂–∞—Ç–∏—è
            compressed_weights[name] = param.half().clone()  # –í–∞–∂–Ω–æ: .clone()
        else:
            compressed_weights[name] = param

    checkpoint['model_state_dict'] = compressed_weights

    # 3. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º —Å–∂–∞—Ç–∏–µ–º
    with gzip.open(path, 'wb', compresslevel=9) as f:
        torch.save(checkpoint, f, pickle_protocol=4)

    # 4. –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–∂–∞—Ç–∏—è
    size_mb = os.path.getsize(path) / 1024 / 1024

    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å —Ä–∞–∑–º–µ—Ä–æ–º –±–µ–∑ —Å–∂–∞—Ç–∏—è
    temp_path = path + '.tmp'
    torch.save(checkpoint, temp_path)  # –ë–µ–∑ —Å–∂–∞—Ç–∏—è
    uncompressed_size = os.path.getsize(temp_path) / 1024 / 1024
    os.remove(temp_path)

    compression_ratio = (1 - size_mb / uncompressed_size) * 100

    print(f"[System]  –ß–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {size_mb:.1f} MB")
    print(f"[System]  –°–∂–∞—Ç–∏–µ: {compression_ratio:.0f}% –æ—Ç {uncompressed_size:.1f} MB")

    return size_mb

def load_compressed_checkpoint(model, path, device):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ–º —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
    """
    try:
        # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º
        with gzip.open(path, 'rb') as f:
            checkpoint = torch.load(f, map_location='cpu', weights_only=False)

        # 2. –ü–æ–ª—É—á–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –≤–µ—Å–∞
        saved_weights = checkpoint['model_state_dict']
        current_weights = model.state_dict()
        loaded_weights = {}

        # 3. –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö
        for name in current_weights.keys():
            if name in saved_weights:
                saved_param = saved_weights[name]
                current_param = current_weights[name]

                if saved_param.is_floating_point() and current_param.is_floating_point():
                    # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π dtype –º–æ–¥–µ–ª–∏
                    loaded_weights[name] = saved_param.to(current_param.dtype)
                else:
                    loaded_weights[name] = saved_param
            else:
                # –ï—Å–ª–∏ –≤–µ—Å–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                loaded_weights[name] = current_weights[name]
                print(f"‚ö†Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä: {name}")

        # 4. –ó–∞–≥—Ä—É–∂–∞–µ–º –≤ –º–æ–¥–µ–ª—å
        model.load_state_dict(loaded_weights)
        model.to(device)

        print(f"‚úì –ß–µ–∫–ø–æ–∏–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω")
        print(f"  –≠–ø–æ—Ö–∞: {checkpoint['epoch']}")
        print(f"  Best loss: {checkpoint['best_loss']:.4f}")
        print(f"  LR: {checkpoint['learning_rate']:.6f}")

        return {
            'model': model,
            'epoch': checkpoint['epoch'],
            'best_loss': checkpoint['best_loss'],
            'learning_rate': checkpoint['learning_rate'],
        }

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
        import traceback
        traceback.print_exc()
        return None

def run_training():
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—á–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π (–ù–û–í–û–ï)
    torch.set_float32_matmul_precision('medium')

    # –í–∫–ª—é—á–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ cuDNN
    torch.backends.cudnn.benchmark = True

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    os.makedirs(os.path.dirname(config.checkpoint_path), exist_ok=True)

    full_classes = sorted(os.listdir(config.source_dir))

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º/–ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º —Å–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Å–æ–≤
    with open(config.labels_path, 'w') as f:
        f.write('\n'.join(full_classes))

    print(f"üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(full_classes)}")

    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø–æ—Ä—è–¥–∫–æ–º –∫–ª–∞—Å—Å–æ–≤
    full_dataset = ImageDataset(config.source_dir)
    full_dataset.classes = full_classes  # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Ä—è–¥–æ–∫

    train_size = int((1 - config.val_split) * len(full_dataset))
    train_ds, val_ds = torch.utils.data.random_split(full_dataset, [train_size, len(full_dataset) - train_size])

    train_loader = DataLoader(
        train_ds,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=os.cpu_count(),
        persistent_workers=True,
        prefetch_factor=2,
        pin_memory=True)
    val_loader = DataLoader(
        val_ds,
        batch_size=config.batch_size,
        num_workers=os.cpu_count(),
        persistent_workers=True,
        prefetch_factor=2,
        pin_memory=True)

    model = AnimeClassifier(len(full_classes)).to(device)

    class_weights = get_class_weights_from_dirs(config.source_dir, full_classes).to(device)

    optimizer = optim.AdamW(model.parameters(), lr=config.lr)
    scaler = torch.amp.GradScaler('cuda', enabled=config.mixed_precision and torch.cuda.is_available())
    start_epoch = 0
    best_loss = float('inf')
    early_stop_counter = 0

    if config.resume_training:
        # –ü—Ä–æ—Å—Ç–æ –∑–∞–≥—Ä—É–∂–∞–µ–º —Å–∂–∞—Ç—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç
        loaded = load_compressed_checkpoint(model, config.checkpoint_path, device)

        if loaded is not None:
            # –ü–æ–ª—É—á–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
            model = loaded['model']

            # –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–º LR
            optimizer = optim.AdamW(model.parameters(), lr=loaded['learning_rate'])

            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è
            start_epoch = loaded['epoch'] + 1
            best_loss = loaded['best_loss']

            scaler = torch.amp.GradScaler('cuda', enabled=config.mixed_precision and torch.cuda.is_available())

            # –ö–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏
            compile_model(model)

            print(f"üîÑ –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å —ç–ø–æ—Ö–∏ {start_epoch}, LR={loaded['learning_rate']:.6f}")
        else:
            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å, –Ω–∞—á–∏–Ω–∞–µ–º —Å –Ω—É–ª—è
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç, –Ω–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è")

            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ
            compile_model(model)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–π —Å–∂–∞—Ç—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç
            save_compressed_checkpoint(
                model=model,
                epoch=-1,
                best_loss=float('inf'),
                lr=config.lr,
                path=config.checkpoint_path
            )
            print("[System]  Initial compressed checkpoint saved")
    else:
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ
        compile_model(model)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–π —Å–∂–∞—Ç—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç
        save_compressed_checkpoint(
            model=model,
            epoch=-1,
            best_loss=float('inf'),
            lr=config.lr,
            path=config.checkpoint_path
        )
        print("[System]  Initial compressed checkpoint saved")

    summary(model, input_size=(1, 3, 224, 224))

    start_time = time.time()  # –ó–∞—Å–µ–∫–∞–µ–º –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞
    optimizer.zero_grad(set_to_none=True)  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
    train_loader_len = len(train_loader)
    val_loader_len = len(val_loader)
    for epoch in range(start_epoch, config.epochs):
        model.train()
        train_loss = 0.0
        train_correct, train_total = 0, 0
        optimizer.zero_grad()

        epoch_start_time = time.time()  # –í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ —ç–ø–æ—Ö–∏
        for batch_idx, (inputs, labels) in enumerate(train_loader):
            batch_start_time = time.time()  # –í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–∞
            inputs, labels = inputs.to(device), labels.to(device)

            outputs, loss = forward_with_mixup_cutmix(model, inputs, labels, config, class_weights, device)
            # –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ (–æ—Å–Ω–æ–≤–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ)
            scaler.scale(loss).backward()
            train_loss += loss.item()

            # –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –∫–ª–∏–ø–ø–∏–Ω–≥
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            # –®–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad(set_to_none=True)

            # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏
            _, predicted = torch.max(outputs, 1)
            current_batch_size = labels.size(0)
            train_total += current_batch_size
            train_correct += (predicted == labels).sum().item()

            # –†–∞—Å—á–µ—Ç –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            batch_duration = time.time() - batch_start_time
            remaining_batches = train_loader_len - (batch_idx + 1)
            estimated_remaining_time = remaining_batches * batch_duration * 3

            remaining_time_str = time.strftime('%H:%M:%S', time.gmtime(estimated_remaining_time))
            print(
                f"\r[Train] Epoch {epoch+1}/{config.epochs} | Batch {batch_idx+1}/{train_loader_len} | "
                f"Loss: {(loss.item()):.4f} | Remaining time: {remaining_time_str}",
                end='', flush=True)

        train_accuracy = 100 * train_correct / train_total
        print()

        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        model.eval()
        val_loss, val_correct, val_total = 0.0, 0, 0
        all_preds, all_labels = [], []

        with torch.inference_mode(), torch.amp.autocast('cuda', enabled=config.mixed_precision):
            for batch_idx, (inputs, labels) in enumerate(val_loader):
                batch_start_time = time.time()

                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)

                loss = focal_loss_with_smoothing(outputs, labels, config.focal_gamma, config.smoothing)
                val_loss += loss.item()

                _, predicted = torch.max(outputs, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()

                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())

                # –†–∞—Å—á–µ—Ç –æ—Å—Ç–∞–≤—à–µ–≥–æ—Å—è –≤—Ä–µ–º–µ–Ω–∏
                batch_duration = time.time() - batch_start_time
                remaining_batches = val_loader_len - (batch_idx + 1)
                estimated_remaining_time = remaining_batches * batch_duration * 3
                remaining_time_str = time.strftime('%H:%M:%S', time.gmtime(estimated_remaining_time))

                print(
                    f"\r[Val]   Epoch {epoch+1}/{config.epochs} | Batch {batch_idx+1}/{val_loader_len} | "
                    f"Loss: {loss.item():.4f} | Remaining: {remaining_time_str}",
                    end='', flush=True)

        print()

        # –£–º–µ–Ω—å—à–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è
        current_lr = optimizer.param_groups[0]['lr']
        if val_loss > best_loss:
            next_lr = current_lr * config.factor_lr
            optimizer = optim.AdamW(model.parameters(), lr=next_lr)
        else:
            next_lr = current_lr
        
        # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
        val_accuracy = 100 * val_correct / val_total
        val_precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)
        val_recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)
        val_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)

        epoch_end_time = time.time()  # –í—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è —ç–ø–æ—Ö–∏
        epoch_duration = epoch_end_time - epoch_start_time
        total_elapsed_time = epoch_end_time - start_time
        epoch_duration_str = time.strftime("%H:%M:%S", time.gmtime(epoch_duration))
        total_elapsed_str = time.strftime("%H:%M:%S", time.gmtime(total_elapsed_time))

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        print(f"[Summary] Train Loss: {train_loss/len(train_loader):.4f} | Acc: {train_accuracy:.2f}%")
        print(f"[Summary] Val   Loss: {val_loss/len(val_loader):.4f} | Acc: {val_accuracy:.2f}%")
        print(f"[Summary] Val Precision: {val_precision:.4f} | Recall: {val_recall:.4f} | F1: {val_f1:.4f}")
        print(f"[Time]    Epoch: {epoch_duration_str} | Total: {total_elapsed_str}")
        print(f"[Summary] LR: {current_lr:.10f}")
        print(f"[Summary] Next LR: {next_lr:.10f}")

        # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
        if val_loss < best_loss:
            best_loss = val_loss
            early_stop_counter = 0
            current_lr = optimizer.param_groups[0]['lr']
            save_compressed_checkpoint(
                model=model,
                epoch=epoch,
                best_loss=best_loss,
                lr=current_lr,
                path=config.checkpoint_path
            )
            print("[System]  Checkpoint saved (compressed)")
        else:
            early_stop_counter += 1
            if early_stop_counter >= config.early_stopping_patience:
                print("[System]  Early Stop")
                break
        print()

# ====================== –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ======================
def convert_to_onnx():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–ª–∞—Å—Å—ã
    with open(config.labels_path) as f:
        classes = [line.strip() for line in f]

    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = AnimeClassifier(len(classes)).to(device)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–∂–∞—Ç—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç
    loaded = load_compressed_checkpoint(model, config.checkpoint_path, device)

    if loaded is None:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ ONNX!")
        return

    model = loaded['model']
    model.eval()

    # –£–¥–∞–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å _orig_mod. –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –±—ã–ª–∞ —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–∞
    model_state_dict = model.state_dict()
    if any('_orig_mod.' in key for key in model_state_dict.keys()):
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –±—ã–ª–∞ —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–∞, –Ω—É–∂–Ω–æ –µ–µ –¥–µ–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞—Ç—å
        model = torch._dynamo.run(model)

    print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –¥–ª—è ONNX —ç–∫—Å–ø–æ—Ä—Ç–∞")
    print(f"   –ö–ª–∞—Å—Å–æ–≤: {len(classes)}")
    print(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

    # –≠–∫—Å–ø–æ—Ä—Ç –≤ ONNX
    dummy_input = torch.randn(1, 3, *config.input_size).to(device)

    try:
        torch.onnx.export(
            model,
            dummy_input,
            config.onnx_path,
            input_names=['input'],
            output_names=['output'],
            dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}},
            do_constant_folding=True,
            opset_version=14,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –Ω–æ–≤—É—é –≤–µ—Ä—Å–∏—é
            training=torch.onnx.TrainingMode.EVAL,
            verbose=False
        )
        print(f"‚úÖ ONNX –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {config.onnx_path}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
        if os.path.exists(config.onnx_path):
            size_mb = os.path.getsize(config.onnx_path) / 1024 / 1024
            print(f"   –†–∞–∑–º–µ—Ä ONNX —Ñ–∞–π–ª–∞: {size_mb:.2f} MB")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ –≤ ONNX: {e}")
        import traceback
        traceback.print_exc()

def test_onnx():
    if not os.path.exists(config.onnx_path):
        print("‚ùå ONNX –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
        print(f"   –ü—É—Ç—å: {config.onnx_path}")
        return

    # –ó–∞–≥—Ä—É–∑–∫–∞ ONNX-–º–æ–¥–µ–ª–∏
    try:
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —Å–µ—Å—Å–∏—é ONNX Runtime
        options = ort.SessionOptions()
        options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã
        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if torch.cuda.is_available() else ['CPUExecutionProvider']

        session = ort.InferenceSession(config.onnx_path, options, providers=providers)
        print("‚úÖ ONNX Runtime —Å–µ—Å—Å–∏—è —Å–æ–∑–¥–∞–Ω–∞")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ ONNX –º–æ–¥–µ–ª–∏: {e}")
        return

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    test_image_path = os.path.join(config.dir, "test.jpg")
    if not os.path.exists(test_image_path):
        print(f"‚ùå –¢–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {test_image_path}")
        print("   –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª test.jpg –≤ –ø–∞–ø–∫–µ –ø—Ä–æ–µ–∫—Ç–∞")
        return

    try:
        img = Image.open(test_image_path).convert('RGB')
        img_np = np.array(img)

        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π
        transform = ImageDataset._get_transforms('val')
        augmented = transform(image=img_np)
        img_tensor = augmented['image'].unsqueeze(0)

        print(f"‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {img.size[0]}x{img.size[1]}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
        return

    # ====================== PyTorch –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ ======================
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–ª–∞—Å—Å—ã
    with open(config.labels_path) as f:
        classes = [line.strip() for line in f]

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –∑–∞–≥—Ä—É–∑–∫–∞ PyTorch-–º–æ–¥–µ–ª–∏
    model = AnimeClassifier(len(classes)).to(device)
    loaded = load_compressed_checkpoint(model, config.checkpoint_path, device)

    if loaded is None:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å PyTorch –º–æ–¥–µ–ª—å –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è!")
        return

    model = loaded['model']
    model.eval()

    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ PyTorch
    with torch.no_grad():
        pytorch_output = model(img_tensor.to(device))
        pytorch_probs = torch.softmax(pytorch_output, dim=1).cpu()

    # ====================== ONNX –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ ======================
    try:
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è ONNX
        onnx_input = img_tensor.numpy().astype(np.float32)

        # –ó–∞–ø—É—Å–∫–∞–µ–º inference
        onnx_outputs = session.run(None, {'input': onnx_input})
        onnx_probs = torch.softmax(torch.tensor(onnx_outputs[0]), dim=1)

        print("‚úÖ ONNX inference –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ ONNX inference: {e}")
        return

    # ====================== –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ======================

    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã PyTorch
    print("\n" + "="*50)
    print("[PyTorch] –¢–æ–ø-5 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
    pytorch_top_probs, pytorch_top_indices = torch.topk(pytorch_probs, 5)
    for i, (prob, idx) in enumerate(zip(pytorch_top_probs[0], pytorch_top_indices[0])):
        print(f"{i+1}. {classes[idx]}: {prob.item()*100:.2f}%")

    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã ONNX
    print("\n[ONNX] –¢–æ–ø-5 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
    onnx_top_probs, onnx_top_indices = torch.topk(onnx_probs, 5)
    for i, (prob, idx) in enumerate(zip(onnx_top_probs[0], onnx_top_indices[0])):
        print(f"{i+1}. {classes[idx]}: {prob.item()*100:.2f}%")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    diff = torch.max(torch.abs(pytorch_probs - onnx_probs)).item()
    print(f"\n[–°—Ä–∞–≤–Ω–µ–Ω–∏–µ] –†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ –º–µ–∂–¥—É –≤—ã—Ö–æ–¥–∞–º–∏: {diff:.6f}")

    if diff < 0.001:
        print("‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞! –†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ < 0.001")
    elif diff < 0.01:
        print("‚ö†Ô∏è  –ù–µ–±–æ–ª—å—à–æ–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ (0.001-0.01), –≤–æ–∑–º–æ–∂–Ω–æ –∏–∑-–∑–∞ —á–∏—Å–ª–µ–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏")
    else:
        print("‚ùå –ë–æ–ª—å—à–æ–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ (> 0.01)! –í–æ–∑–º–æ–∂–Ω–∞—è –æ—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏")

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    print("\n" + "="*50)
    print(f"PyTorch device: {device}")
    print(f"PyTorch dtype: {pytorch_probs.dtype}")
    print(f"ONNX dtype: {onnx_probs.dtype}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(classes)}")

def get_classes():
  with open(config.labels_path) as f:
      return [line.strip() for line in f]

# ====================== –ò–ù–¢–ï–†–§–ï–ô–° ======================
def main_menu():
    while True:
        print("\n–ú–µ–Ω—é:")
        print("1. –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å")
        print("2. –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ")  # –ù–æ–≤–∞—è –æ–ø—Ü–∏—è
        print("3. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ ONNX")
        print("4. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å ONNX")
        print("0. –í—ã—Ö–æ–¥")
        choice = input("–í—ã–±–æ—Ä: ").strip()

        if choice == '1':
            config.resume_training = False
            run_training()
        elif choice == '2':
            if not os.path.exists(config.checkpoint_path):
                print("‚ùå –ß–µ–∫–ø–æ–∏–Ω—Ç –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω!")
                continue
            config.resume_training = True
            run_training()
        elif choice == '3':
            convert_to_onnx()
        elif choice == '4':
            test_onnx()
        elif choice == '0':
            break
        else:
            print("–ù–µ–≤–µ—Ä–Ω—ã–π –≤–≤–æ–¥!")

if __name__ == "__main__":
    main_menu()